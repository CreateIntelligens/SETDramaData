#!/usr/bin/env python3
"""
Pyannote Speaker Segmentation with Global Speaker Database
èªªè©±äººç´šåˆ¥åˆ†æ®µç³»çµ± - ä½¿ç”¨å®˜æ–¹æ­£è¦é›¢ç·šæ–¹æ³•
"""

import os
import sys
from pathlib import Path

# å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„
project_root = Path(__file__).parent.parent

# è¼‰å…¥ .env æª”æ¡ˆ
def load_env_file():
    """è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸"""
    env_file = project_root / ".env"
    if env_file.exists():
        print(f"ğŸ“ è¼‰å…¥ç’°å¢ƒè®Šæ•¸: {env_file}")
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    os.environ[key] = value
                    if key in ['SIMILARITY_THRESHOLD', 'MIN_SPEAKER_DURATION', 'VOICE_ACTIVITY_THRESHOLD']:
                        print(f"  âœ… {key}={value}")
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ° .env æª”æ¡ˆ: {env_file}")

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_env_file()

# åŒ¯å…¥æ­£è¦é›¢ç·šè¼‰å…¥æ¨¡çµ„
try:
    from offline_pipeline import load_offline_pipeline
    OFFICIAL_OFFLINE_AVAILABLE = True
    print("ğŸ¯ ä½¿ç”¨å®˜æ–¹æ­£è¦é›¢ç·šæ–¹æ³•")
except ImportError:
    OFFICIAL_OFFLINE_AVAILABLE = False
    print("âš ï¸ æ­£è¦é›¢ç·šæ¨¡çµ„ä¸å¯ç”¨ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•")
    
    # å‚™ç”¨ï¼šèˆŠçš„é›¢ç·šè¨­å®š
    models_dir = project_root / "models" / "huggingface"
    
    # å¼·åˆ¶é›¢ç·šæ¨¡å¼ç’°å¢ƒè®Šæ•¸
    os.environ.update({
        'TRANSFORMERS_OFFLINE': '1',
        'HUGGINGFACE_HUB_OFFLINE': '1',
        'HF_HUB_OFFLINE': '1'
    })
    
    print(f"ğŸ”§ æ¨¡å‹ç›®éŒ„: {models_dir}")
    print(f"ğŸ”§ å®Œå…¨é›¢ç·šæ¨¡å¼: å·²å•Ÿç”¨")

# è¨˜æ†¶é«”å„ªåŒ–è¨­å®š
os.environ.update({
    'MKL_SERVICE_FORCE_INTEL': '1',
    'MKL_THREADING_LAYER': 'GNU',
    'OMP_NUM_THREADS': '4',
    'MKL_NUM_THREADS': '4',
    'KMP_DUPLICATE_LIB_OK': 'TRUE',
    'TORCH_WARN': '0'
})

import torch
import numpy as np
import argparse
from typing import List, Tuple, Dict
import librosa
import soundfile as sf
from tqdm import tqdm
import warnings
import logging
import gc

# éœéŸ³è­¦å‘Š
warnings.filterwarnings("ignore")
logging.getLogger("torch").setLevel(logging.ERROR)

# PyTorch è¨˜æ†¶é«”å„ªåŒ–
torch.set_num_threads(2)
torch.set_num_interop_threads(1)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

# GPU è¨˜æ†¶é«”è¨­å®š
if torch.cuda.is_available():
    print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    torch.cuda.empty_cache()
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {gpu_memory:.1f}GB (è‡ªå‹•ç®¡ç†)")
else:
    print("âš ï¸ ä½¿ç”¨ CPU")

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘ï¼ˆç”¨æ–¼å°å…¥ fix_symlinksï¼‰
sys.path.append(str(project_root))

from speaker_database import SpeakerDatabase
from speaker_level_segmentation import segment_by_speaker_level_approach

# Pyannote imports
try:
    from pyannote.audio import Pipeline, Model
    from pyannote.audio.pipelines.utils.hook import ProgressHook
    from pyannote.core import Annotation
    print("âœ… Pyannote æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ Pyannote è¼‰å…¥å¤±æ•—: {e}")
    sys.exit(1)


class EmbeddingInference:
    """Embedding model wrapper - ä½¿ç”¨æ­£è¦é›¢ç·šæ–¹æ³•"""

    def __init__(self, device: torch.device, pipeline=None):
        self.device = device
        self.model = None
        self.pipeline = pipeline
        self._load_model()

    def _load_model(self):
        """è¼‰å…¥ embedding æ¨¡å‹"""
        try:
            if OFFICIAL_OFFLINE_AVAILABLE and self.pipeline:
                # ğŸ¯ ä½¿ç”¨æ­£è¦é›¢ç·šæ–¹æ³•ï¼šå¾ pipeline ä¸­å–å¾— embedding æ¨¡å‹
                print("ğŸ“ ä½¿ç”¨æ­£è¦é›¢ç·šæ–¹æ³•è¼‰å…¥ embedding æ¨¡å‹...")
                print(f"ğŸ” Pipeline å±¬æ€§: {[attr for attr in dir(self.pipeline) if 'embed' in attr.lower()]}")
                
                # å˜—è©¦ä¸åŒçš„å±¬æ€§åç¨±
                for attr_name in ['_embedding', 'embedding', '_embedding_model', 'embedding_model']:
                    if hasattr(self.pipeline, attr_name):
                        embedding_model = getattr(self.pipeline, attr_name)
                        if embedding_model is not None:
                            self.model = embedding_model
                            print(f"âœ… å¾ Pipeline.{attr_name} å–å¾— Embedding æ¨¡å‹æˆåŠŸ")
                            return
                
                print("âš ï¸ Pipeline ä¸­æ²’æœ‰æ‰¾åˆ° embedding æ¨¡å‹ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•...")
            
            # å‚™ç”¨æ–¹æ³•ï¼šå‚³çµ±è¼‰å…¥
            print("ğŸ“ ä½¿ç”¨å‚™ç”¨æ–¹æ³•è¼‰å…¥ embedding æ¨¡å‹...")
            from pyannote.audio import Model
            
            if OFFICIAL_OFFLINE_AVAILABLE:
                # å˜—è©¦å¾æ­£è¦é›¢ç·šæ¨¡å‹æª”æ¡ˆè¼‰å…¥
                project_root = Path(__file__).parent.parent
                model_path = project_root / "models" / "pyannote_model_wespeaker-voxceleb-resnet34-LM.bin"
                
                if model_path.exists():
                    print(f"ğŸ“ è¼‰å…¥æ­£è¦é›¢ç·šæ¨¡å‹: {model_path}")
                    try:
                        from pyannote.audio import Model
                        # å˜—è©¦ç›´æ¥è¼‰å…¥ .bin æª”æ¡ˆ
                        self.model = Model.from_pretrained(str(model_path)).to(self.device)
                        self.model.eval()
                        print("âœ… æ­£è¦é›¢ç·š Embedding æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                        return
                    except Exception as e:
                        print(f"âš ï¸ ç›´æ¥è¼‰å…¥ .bin æª”æ¡ˆå¤±æ•—: {e}")
                        # ç¹¼çºŒä½¿ç”¨å‚™ç”¨æ–¹æ³•
            
            # æœ€å¾Œçš„å‚™ç”¨æ–¹æ³•
            project_root = Path(__file__).parent.parent
            models_dir = project_root / "models" / "huggingface"
            
            print(f"ğŸ“ å¿«å–ç›®éŒ„: {models_dir}")
            self.model = Model.from_pretrained(
                "pyannote/wespeaker-voxceleb-resnet34-LM",
                cache_dir=str(models_dir),
                local_files_only=True,
                use_auth_token=None
            ).to(self.device)
            self.model.eval()
            print("âœ… Embedding æ¨¡å‹è¼‰å…¥æˆåŠŸ")

        except Exception as e:
            print(f"âŒ Embedding æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            # å¦‚æœæ˜¯æ­£è¦é›¢ç·šç’°å¢ƒï¼Œé€™å¯èƒ½ä¸æ˜¯è‡´å‘½éŒ¯èª¤
            if OFFICIAL_OFFLINE_AVAILABLE and self.pipeline:
                print("ğŸ’¡ æ­£è¦é›¢ç·šç’°å¢ƒä¸­ï¼Œembedding åŠŸèƒ½å·²æ•´åˆåœ¨ Pipeline ä¸­")
                self.model = None  # è¨­ç‚º Noneï¼Œå¾ŒçºŒæœƒæª¢æŸ¥
            else:
                raise


def load_subtitles(subtitle_file: str, fps: float = 30.0) -> List[Tuple[float, str]]:
    """è¼‰å…¥å­—å¹•æª”æ¡ˆ"""
    subtitles = []

    def parse_timecode(timecode_str: str) -> float:
        """è§£ææ™‚é–“ç¢¼"""
        try:
            timecode_str = timecode_str.lstrip('\ufeff')

            try:
                return float(timecode_str)
            except ValueError:
                pass

            if ':' in timecode_str:
                parts = timecode_str.split(':')
                if len(parts) == 4:  # HH:MM:SS:FF
                    hours, minutes, seconds, frames = map(int, parts)
                    return hours * 3600 + minutes * 60 + seconds + frames / fps
                elif len(parts) == 3:
                    if int(parts[0]) > 59:  # HH:MM:SS
                        hours, minutes, seconds = map(int, parts)
                        return hours * 3600 + minutes * 60 + seconds
                    else:  # MM:SS:FF
                        minutes, seconds, frames = map(int, parts)
                        return minutes * 60 + seconds + frames / fps

            raise ValueError(f"ä¸æ”¯æ´çš„æ™‚é–“ç¢¼æ ¼å¼: {timecode_str}")

        except Exception as e:
            raise ValueError(f"ç„¡æ³•è§£ææ™‚é–“ç¢¼ '{timecode_str}': {e}")

    try:
        with open(subtitle_file, 'r', encoding='utf-8-sig') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                try:
                    parts = line.split(' ', 1)
                    if len(parts) >= 1:
                        timecode_str = parts[0]
                        text = parts[1] if len(parts) > 1 else ""
                        timestamp = parse_timecode(timecode_str)
                        subtitles.append((timestamp, text))
                except ValueError as e:
                    print(f"âš ï¸ ç¬¬ {line_num} è¡Œè§£æå¤±æ•—: {e}")
                    continue

        print(f"âœ… è¼‰å…¥ {len(subtitles)} å€‹å­—å¹•")
        return subtitles

    except Exception as e:
        print(f"âŒ å­—å¹•è¼‰å…¥å¤±æ•—: {e}")
        return []


def perform_speaker_diarization(audio_file: str, pipeline, device: torch.device) -> Annotation:
    """åŸ·è¡Œèªªè©±äººåˆ†é›¢"""
    print(f"ğŸµ è™•ç†éŸ³æª”: {audio_file}")

    try:
        # ç°¡å–®è¨˜æ†¶é«”æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # é¡¯ç¤ºè¨˜æ†¶é«”ç‹€æ…‹
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ“Š GPU è¨˜æ†¶é«”: å·²åˆ†é… {allocated:.2f}GB, å¿«å– {cached:.2f}GB")

        print("ğŸš€ åŸ·è¡Œ diarization...")
        with ProgressHook() as hook:
            diarization = pipeline(audio_file, hook=hook)

        # çµ±è¨ˆçµæœ
        speakers = set()
        total_duration = 0.0
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speakers.add(speaker)
            total_duration += turn.duration

        print(f"âœ… å®Œæˆ: {len(speakers)} å€‹èªªè©±äºº, {total_duration:.1f}s")
        
        # é¡¯ç¤ºæœ€çµ‚è¨˜æ†¶é«”ç‹€æ…‹
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ“Š å®Œæˆå¾Œ GPU è¨˜æ†¶é«”: å·²åˆ†é… {allocated:.2f}GB, å¿«å– {cached:.2f}GB")
        
        return diarization

    except Exception as e:
        print(f"âŒ Diarization å¤±æ•—: {e}")
        raise




def segment_audio_files(segments, audio_path, output_dir, subtitles, episode_num):
    """åˆ†å‰²ä¸¦å„²å­˜éŸ³æª”"""
    print("ğŸ“ åˆ†å‰²éŸ³æª”...")

    try:
        audio, sr = librosa.load(audio_path, sr=None)
        print(f"âœ… éŸ³æª”è¼‰å…¥: {len(audio)/sr:.1f}s, {sr}Hz")
    except Exception as e:
        print(f"âŒ éŸ³æª”è¼‰å…¥å¤±æ•—: {e}")
        return

    os.makedirs(output_dir, exist_ok=True)
    saved_count = 0

    for i, (start, end, speaker_id) in enumerate(tqdm(segments, desc="å„²å­˜ç‰‡æ®µ")):
        try:
            start_sample = int(start * sr)
            end_sample = int(end * sr)
            segment_audio = audio[start_sample:end_sample]

            if len(segment_audio) == 0:
                continue

            # å°‹æ‰¾å­—å¹•
            segment_text = ""
            for timestamp, text in subtitles:
                if start <= timestamp < end:
                    segment_text += text + " "
            segment_text = segment_text.strip()

            if not segment_text:
                continue

            # å»ºç«‹æª”æ¡ˆè·¯å¾‘
            chapter_id = episode_num
            paragraph_id = i + 1
            sentence_id = 1

            utterance_id = f"{speaker_id:03d}_{chapter_id:03d}_{paragraph_id:06d}_{sentence_id:06d}"

            speaker_dir = os.path.join(output_dir, f"{speaker_id:03d}")
            chapter_dir = os.path.join(speaker_dir, f"{chapter_id:03d}")
            os.makedirs(chapter_dir, exist_ok=True)

            # å„²å­˜éŸ³æª”
            audio_filename = f"{utterance_id}.wav"
            audio_filepath = os.path.join(chapter_dir, audio_filename)
            sf.write(audio_filepath, segment_audio, sr)

            # å„²å­˜å­—å¹•
            text_filename = f"{utterance_id}.normalized.txt"
            text_filepath = os.path.join(chapter_dir, text_filename)
            with open(text_filepath, 'w', encoding='utf-8') as f:
                f.write(segment_text)

            saved_count += 1

        except Exception as e:
            print(f"âš ï¸ ç‰‡æ®µ {i} å„²å­˜å¤±æ•—: {e}")
            continue

    print(f"âœ… å„²å­˜ {saved_count} å€‹ç‰‡æ®µåˆ° {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Pyannote Speaker Segmentation")
    parser.add_argument("audio_file", help="éŸ³æª”è·¯å¾‘")
    parser.add_argument("subtitle_file", help="å­—å¹•æª”è·¯å¾‘")
    parser.add_argument("--episode_num", type=int, required=True, help="é›†æ•¸")
    parser.add_argument("--output_dir", default="output", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--min_duration", type=float, default=1.0, help="æœ€å°ç‰‡æ®µé•·åº¦")
    parser.add_argument("--max_duration", type=float, default=15.0, help="æœ€å¤§ç‰‡æ®µé•·åº¦")
    parser.add_argument("--similarity_threshold", type=float, 
                        default=float(os.environ.get('SIMILARITY_THRESHOLD', '0.40')), 
                        help="ç›¸ä¼¼åº¦é–¾å€¼")
    parser.add_argument("--voice_activity_threshold", type=float, 
                        default=float(os.environ.get('VOICE_ACTIVITY_THRESHOLD', '0.1')), 
                        help="èªéŸ³æ´»å‹•é–¾å€¼")
    parser.add_argument("--min_speaker_duration", type=float, 
                        default=float(os.environ.get('MIN_SPEAKER_DURATION', '5.0')), 
                        help="æœ€å°èªªè©±äººæ™‚é•·")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶é‡æ–°è™•ç†")
    parser.add_argument("--device", choices=["cpu", "cuda"], 
                       default="cuda" if torch.cuda.is_available() else "cpu", help="è£ç½®")

    args = parser.parse_args()

    device = torch.device(args.device)
    print(f"ğŸ”§ ä½¿ç”¨è£ç½®: {device}")

    # æª¢æŸ¥æª”æ¡ˆ
    if not os.path.exists(args.audio_file):
        print(f"âŒ éŸ³æª”ä¸å­˜åœ¨: {args.audio_file}")
        sys.exit(1)

    if not os.path.exists(args.subtitle_file):
        print(f"âŒ å­—å¹•æª”ä¸å­˜åœ¨: {args.subtitle_file}")
        sys.exit(1)

    # åˆå§‹åŒ–è³‡æ–™åº«
    print("1. åˆå§‹åŒ–è³‡æ–™åº«...")
    db = SpeakerDatabase()

    # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
    processed_episodes = db.get_processed_episodes()
    if args.episode_num in processed_episodes and not args.force:
        print(f"âš ï¸ é›†æ•¸ {args.episode_num} å·²è™•ç†éï¼Œä½¿ç”¨ --force é‡æ–°è™•ç†")
        sys.exit(0)

    # è¼‰å…¥å­—å¹•
    print("2. è¼‰å…¥å­—å¹•...")
    subtitles = load_subtitles(args.subtitle_file)
    if not subtitles:
        print("âŒ å­—å¹•è¼‰å…¥å¤±æ•—")
        sys.exit(1)

    # è·³éç¬¦è™Ÿé€£çµä¿®å¾©ï¼ˆä½¿ç”¨æ­£è¦é›¢ç·šæ–¹æ³•ä¸éœ€è¦ï¼‰
    print("3. æ­£è¦é›¢ç·šæ–¹æ³•ä¸éœ€è¦ç¬¦è™Ÿé€£çµä¿®å¾©ï¼Œè·³éæ­¤æ­¥é©Ÿ...")

    # è¼‰å…¥ diarization pipeline
    print("4. è¼‰å…¥ diarization pipeline...")
    try:
        if OFFICIAL_OFFLINE_AVAILABLE:
            # ğŸ¯ ä½¿ç”¨å®˜æ–¹æ­£è¦é›¢ç·šæ–¹æ³•
            print("ä½¿ç”¨å®˜æ–¹æ­£è¦é›¢ç·šè¼‰å…¥æ–¹æ³•...")
            diarization_pipeline, device_type = load_offline_pipeline()
            print(f"âœ… æ­£è¦é›¢ç·š Pipeline è¼‰å…¥æˆåŠŸ ({device_type.upper()})")
            
            # å¦‚æœéœ€è¦æŒ‡å®šä¸åŒè¨­å‚™
            if str(device) != device_type:
                diarization_pipeline.to(device)
                print(f"ğŸ“± Pipeline å·²ç§»è‡³ {device}")
                
        else:
            # ğŸ”„ å‚™ç”¨æ–¹æ³•ï¼šä½¿ç”¨å¿«å–ç›®éŒ„
            print(f"ğŸ“ ä½¿ç”¨å‚™ç”¨æ–¹æ³•ï¼Œå¿«å–ç›®éŒ„: {models_dir}")
            from pyannote.audio import Pipeline
            diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                cache_dir=str(models_dir),
                use_auth_token=None
            ).to(device)
            print("âœ… å‚™ç”¨æ–¹æ³• Pipeline è¼‰å…¥æˆåŠŸ")

        # è¨˜æ†¶é«”æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"âŒ Pipeline è¼‰å…¥å¤±æ•—: {e}")
        if OFFICIAL_OFFLINE_AVAILABLE:
            print("ğŸ’¡ æ­£è¦æ–¹æ³•è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ models/config.yaml å’Œæ¨¡å‹æª”æ¡ˆ")
        else:
            print(f"ğŸ’¡ å¿«å–ç›®éŒ„: {models_dir}")
        sys.exit(1)

    # åŸ·è¡Œ diarization
    print("5. åŸ·è¡Œ speaker diarization...")
    diarization = perform_speaker_diarization(args.audio_file, diarization_pipeline, device)

    # è¼‰å…¥ embedding model (åœ¨é‡‹æ”¾ pipeline å‰)
    print("6. è¼‰å…¥ embedding æ¨¡å‹...")
    
    try:
        # ğŸ¯ åœ¨æ‰€æœ‰æ¨¡å¼ä¸‹éƒ½è¼‰å…¥ embedding æ¨¡å‹ï¼Œå› ç‚ºå¾ŒçºŒè™•ç†éœ€è¦ç”¨åˆ°
        embedding_inference = EmbeddingInference(device, pipeline=diarization_pipeline if OFFICIAL_OFFLINE_AVAILABLE else None)
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ Embedding æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        sys.exit(1)

    # é‡‹æ”¾ pipeline è¨˜æ†¶é«”
    print("ğŸ§¹ é‡‹æ”¾ pipeline è¨˜æ†¶é«”...")
    del diarization_pipeline
    for _ in range(3):
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # åŸ·è¡Œèªªè©±äººç´šåˆ¥åˆ†æ®µ
    print("7. åŸ·è¡Œèªªè©±äººç´šåˆ¥åˆ†æ®µ...")
    embedding_model = embedding_inference.model if embedding_inference else None
    segments, local_to_global_map = segment_by_speaker_level_approach(
        diarization, subtitles, args.audio_file, embedding_model, device,
        db, args.episode_num, args.min_duration, args.max_duration,
        args.similarity_threshold, args.min_speaker_duration
    )
    print(f"âœ… å»ºç«‹ {len(segments)} å€‹åˆ†æ®µ")

    if not segments:
        print("âŒ æ²’æœ‰æœ‰æ•ˆåˆ†æ®µ")
        sys.exit(1)

    # åˆ†å‰²éŸ³æª”
    segment_audio_files(segments, args.audio_file, args.output_dir, subtitles, args.episode_num)

    # æ¨™è¨˜ç‚ºå·²è™•ç†
    db.mark_episode_processed(args.episode_num)

    # é¡¯ç¤ºçµ±è¨ˆ
    print("\n" + "="*60)
    print("ğŸ“Š è™•ç†æ‘˜è¦")
    print("="*60)
    print(f"é›†æ•¸: {args.episode_num}")
    print(f"ç¸½åˆ†æ®µ: {len(segments)}")
    print(f"èªªè©±äººæ•¸: {len(set(seg[2] for seg in segments))}")

    if local_to_global_map:
        print(f"\nèªªè©±äººå°æ‡‰:")
        for local_label, global_id in sorted(local_to_global_map.items()):
            print(f"  {local_label} â†’ å…¨åŸŸèªªè©±äºº {global_id}")

    db_stats = db.get_database_stats()
    print(f"\nè³‡æ–™åº«çµ±è¨ˆ:")
    print(f"  ç¸½èªªè©±äºº: {db_stats['total_speakers']}")
    print(f"  ç¸½é›†æ•¸: {db_stats['total_episodes']}")
    print(f"  ç¸½åˆ†æ®µ: {db_stats['total_segments']}")

    print(f"\nâœ… è™•ç†å®Œæˆï¼")
    print(f"è¼¸å‡ºç›®éŒ„: {args.output_dir}")


if __name__ == "__main__":
    main()
