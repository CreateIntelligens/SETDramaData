#!/usr/bin/env python3
"""
æ··åˆåˆ†æ®µæ¨¡å¼ - çµåˆ Pyannote Diarization èˆ‡å­—å¹•æ™‚é–“é»
- ä½¿ç”¨ diarization æä¾›ç²¾ç¢ºçš„èªªè©±äººè®ŠåŒ–é»
- ä½¿ç”¨å­—å¹•æ™‚é–“è»¸ç¢ºä¿ä¸æœƒéºæ¼ä»»ä½•å­—å¹•å…§å®¹
- å„ªå…ˆä¿è­‰å­—å¹•å®Œæ•´æ€§ï¼ŒåŒæ™‚åˆ©ç”¨ diarization çš„èªªè©±äººè¾¨è­˜ç²¾åº¦
"""

import os
import numpy as np
from typing import List, Tuple, Dict, Optional
import torch
from tqdm import tqdm
import librosa
from pyannote.core import Annotation, Segment

def segment_by_hybrid_approach(
    diarization: Annotation,
    subtitles: List[Tuple[float, str]],
    audio_path: str,
    embedding_model,
    device: torch.device,
    db,
    episode_num: int,
    min_duration: float = 1.0,
    max_duration: float = 15.0,
    similarity_threshold: float = 0.30,
    voice_activity_threshold: float = 0.1
) -> Tuple[List[Tuple[float, float, int]], Dict[str, int]]:
    """
    æ··åˆåˆ†æ®µæ–¹æ³•ï¼šçµåˆ diarization çš„èªªè©±äººè®ŠåŒ–é»èˆ‡å­—å¹•æ™‚é–“è»¸
    
    ç­–ç•¥ï¼š
    1. ä»¥å­—å¹•æ™‚é–“é»ç‚ºä¸»è¦åˆ†æ®µåŸºç¤ï¼ˆç¢ºä¿ä¸éºæ¼å…§å®¹ï¼‰
    2. åœ¨å­—å¹•æ®µè½å…§éƒ¨ä½¿ç”¨ diarization æä¾›çš„èªªè©±äººæ¨™ç±¤
    3. åˆä½µç›¸é„°çš„åŒèªªè©±äººç‰‡æ®µ
    4. æœ€çµ‚åˆ†é…å…¨åŸŸ speaker ID
    """
    
    if not subtitles:
        print("   âŒ æ²’æœ‰å­—å¹•è³‡æ–™")
        return [], {}
    
    if not diarization:
        print("   âŒ æ²’æœ‰ diarization çµæœ")
        return [], {}
    
    print(f"   ğŸ¯ æ··åˆåˆ†æ®µæ¨¡å¼ï¼š{len(subtitles)} å¥å­—å¹• + diarization èªªè©±äººè³‡è¨Š")
    
    # Step 1: å»ºç«‹å­—å¹•å°å‘çš„åˆå§‹åˆ†æ®µï¼Œä¸¦åŠ ä¸Š diarization çš„èªªè©±äººæ¨™ç±¤
    subtitle_segments_with_speakers = create_subtitle_segments_with_diarization(
        subtitles, diarization, audio_path, voice_activity_threshold
    )
    
    print(f"   âœ‚ï¸ å»ºç«‹äº† {len(subtitle_segments_with_speakers)} å€‹æ··åˆåˆ†æ®µ")
    
    # Step 2: ç‚ºæ¯å€‹åˆ†æ®µæå– embedding
    segments_with_embeddings = extract_embeddings_for_hybrid_segments(
        subtitle_segments_with_speakers, audio_path, embedding_model, device
    )
    
    # çµ±è¨ˆ embedding æå–æˆåŠŸç‡
    successful_embeddings = sum(1 for seg in segments_with_embeddings if seg.get('embedding_extracted', False))
    print(f"   ğŸ§¬ æå–äº† {len(segments_with_embeddings)} å€‹ embedding (æˆåŠŸ: {successful_embeddings}, å¤±æ•—: {len(segments_with_embeddings) - successful_embeddings})")
    
    # Step 3: åŸºæ–¼èªªè©±äººæ¨™ç±¤å’Œ embedding ç›¸ä¼¼åº¦åˆä½µç›¸é„°ç‰‡æ®µ
    merged_segments = merge_segments_by_speaker_and_similarity(
        segments_with_embeddings, similarity_threshold, max_duration
    )
    print(f"   ğŸ”— åˆä½µå¾Œå‰©é¤˜ {len(merged_segments)} å€‹æ®µè½")
    
    # Step 4: éæ¿¾é•·åº¦ä¸¦åˆ†é… global speaker ID
    final_segments, local_to_global_map = assign_global_speaker_ids_hybrid(
        merged_segments, db, episode_num, min_duration, max_duration, similarity_threshold
    )
    
    print(f"   âœ… æœ€çµ‚ç”¢ç”Ÿ {len(final_segments)} å€‹æœ‰æ•ˆæ®µè½")
    return final_segments, local_to_global_map


def create_subtitle_segments_with_diarization(
    subtitles: List[Tuple[float, str]],
    diarization: Annotation,
    audio_path: str,
    voice_activity_threshold: float = 0.1
) -> List[Dict]:
    """
    çµåˆå­—å¹•æ™‚é–“é»å’Œ diarization èªªè©±äººæ¨™ç±¤å‰µå»ºæ··åˆåˆ†æ®µ
    
    é‚è¼¯ï¼š
    1. ä»¥å­—å¹•ç‚ºä¸»è¦æ™‚é–“è»¸
    2. å°æ¯å€‹å­—å¹•æ™‚é–“é»ï¼ŒæŸ¥æ‰¾ diarization ä¸­çš„ä¸»è¦èªªè©±äºº
    3. ä¿æŒå­—å¹•å®Œæ•´æ€§çš„åŒæ™‚åŠ å…¥èªªè©±äººè³‡è¨Š
    """
    print("   ğŸ”Š Loading audio for hybrid segmentation...")
    try:
        waveform, sr = librosa.load(audio_path, sr=16000)
        audio_duration = waveform.shape[0] / sr
    except Exception as e:
        print(f"   âŒ Error loading audio: {e}")
        audio_duration = 3600.0  # å‡è¨­æœ€é•· 1 å°æ™‚
    
    segments = []
    print("   ğŸ­ çµåˆå­—å¹•èˆ‡ diarization èªªè©±äººè³‡è¨Š...")
    
    filtered_count = 0  # çµ±è¨ˆè¢«éæ¿¾çš„ç‰‡æ®µæ•¸é‡
    
    for i, (start_time, text) in enumerate(tqdm(subtitles, desc="   Processing subtitles", unit="line", ncols=80)):
        # è¨ˆç®—çµæŸæ™‚é–“ (ä½ çš„åŸå§‹é‚è¼¯æ˜¯æ­£ç¢ºçš„)
        if i < len(subtitles) - 1:
            end_time = subtitles[i + 1][0]  # ä¸‹ä¸€æ®µé–‹å§‹æ™‚é–“ = é€™æ®µçµæŸæ™‚é–“
        else:
            end_time = audio_duration
        
        # é™åˆ¶å–®å€‹æ®µè½æœ€å¤§é•·åº¦
        end_time = min(end_time, start_time + 20.0)
        
        if end_time <= start_time:
            filtered_count += 1
            continue
        
        # ç°¡åŒ–é‚è¼¯ï¼šç›´æ¥ç”¨å­—å¹•æ™‚é–“ï¼ŒåªåšåŸºæœ¬æª¢æŸ¥
        if end_time > start_time + 0.5:  # è‡³å°‘0.5ç§’é•·åº¦
            # åœ¨é€™å€‹æ™‚é–“ç¯„åœå…§æ‰¾åˆ°ä¸»è¦çš„èªªè©±äºº
            segment_range = Segment(start_time, end_time)
            dominant_speaker = get_dominant_speaker_in_range(diarization, segment_range)
            
            # åªè¦æœ‰èªªè©±äººå°±ä¿ç•™ï¼ˆç§»é™¤VADè¤‡é›œæª¢æŸ¥ï¼‰
            if dominant_speaker is not None:
                segments.append({
                    'start': start_time,
                    'end': end_time,
                    'text': text,
                    'diarization_speaker': dominant_speaker,
                    'original_index': i,
                    'duration': end_time - start_time,
                    'overlap_ratio': 1.0  # ç°¡åŒ–ï¼šå‡è¨­éƒ½æ˜¯æœ‰æ•ˆèªéŸ³
                })
            else:
                filtered_count += 1
        else:
            filtered_count += 1
    
    print(f"   ğŸ“Š éæ¿¾çµ±è¨ˆ: åŸå§‹å­—å¹• {len(subtitles)} è¡Œ, ä¿ç•™ {len(segments)} è¡Œ, éæ¿¾ {filtered_count} è¡Œ (ç„¡èªéŸ³æ´»å‹•)")
    return segments


def find_actual_speech_range_from_start_time(
    diarization: Annotation,
    subtitle_start_time: float,
    max_search_duration: float = 10.0
) -> Optional[Tuple[float, float, str]]:
    """
    å¾å­—å¹•é–‹å§‹æ™‚é–“é»æ‰¾åˆ°å°æ‡‰çš„å¯¦éš›èªéŸ³ç¯„åœ
    è¿”å›: (èªéŸ³é–‹å§‹æ™‚é–“, èªéŸ³çµæŸæ™‚é–“, èªªè©±è€…) æˆ– None
    
    ç­–ç•¥ï¼š
    1. å¾å­—å¹•é–‹å§‹æ™‚é–“é»é–‹å§‹æœå°‹
    2. æ‰¾åˆ°ç¬¬ä¸€å€‹åŒ…å«è©²æ™‚é–“é»çš„èªéŸ³æ®µ
    3. å¦‚æœæ²’æœ‰ç›´æ¥åŒ…å«ï¼Œæ‰¾è·é›¢æœ€è¿‘çš„èªéŸ³æ®µï¼ˆåœ¨æœå°‹ç¯„åœå…§ï¼‰
    """
    best_match = None
    min_distance = float('inf')
    
    # æœå°‹ç¯„åœ
    search_start = subtitle_start_time
    search_end = subtitle_start_time + max_search_duration
    
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speech_start = turn.start
        speech_end = turn.end
        
        # æƒ…æ³1ï¼šå­—å¹•æ™‚é–“é»åœ¨èªéŸ³æ®µå…§ (æœ€ä½³åŒ¹é…)
        if speech_start <= subtitle_start_time <= speech_end:
            return (speech_start, speech_end, speaker)
        
        # æƒ…æ³2ï¼šèªéŸ³æ®µåœ¨æœå°‹ç¯„åœå…§ï¼Œè¨ˆç®—è·é›¢
        if (speech_start <= search_end and speech_end >= search_start):
            # è¨ˆç®—å­—å¹•é–‹å§‹æ™‚é–“èˆ‡èªéŸ³æ®µçš„è·é›¢
            if subtitle_start_time < speech_start:
                distance = speech_start - subtitle_start_time  # èªéŸ³åœ¨å¾Œ
            elif subtitle_start_time > speech_end:
                distance = subtitle_start_time - speech_end     # èªéŸ³åœ¨å‰
            else:
                distance = 0  # é‡ç–Šï¼ˆå·²åœ¨æƒ…æ³1è™•ç†ï¼‰
            
            # æ›´æ–°æœ€ä½³åŒ¹é…
            if distance < min_distance:
                min_distance = distance
                best_match = (speech_start, speech_end, speaker)
    
    # å¦‚æœæ‰¾åˆ°åˆç†è·é›¢å…§çš„åŒ¹é…ï¼ˆ2ç§’å…§ï¼‰
    if best_match and min_distance <= 2.0:
        return best_match
    
    return None


def calculate_diarization_overlap(diarization: Annotation, segment: Segment) -> float:
    """
    è¨ˆç®—å­—å¹•ç‰‡æ®µèˆ‡ diarization çµæœçš„é‡ç–Šæ¯”ä¾‹
    è¿”å›å€¼ç‚º 0.0-1.0ï¼Œè¡¨ç¤ºæœ‰å¤šå°‘æ¯”ä¾‹çš„æ™‚é–“æœ‰èªéŸ³æ´»å‹•
    """
    total_overlap_duration = 0.0
    segment_duration = segment.duration
    
    if segment_duration <= 0:
        return 0.0
    
    # éæ­·æ‰€æœ‰ diarization ç‰‡æ®µï¼Œè¨ˆç®—é‡ç–Šæ™‚é–“
    for turn, _, _ in diarization.itertracks(yield_label=True):
        overlap = segment & turn
        if overlap:
            total_overlap_duration += overlap.duration
    
    # è¿”å›é‡ç–Šæ¯”ä¾‹
    overlap_ratio = total_overlap_duration / segment_duration
    return min(1.0, overlap_ratio)  # é™åˆ¶åœ¨ 1.0 ä»¥å…§


def get_dominant_speaker_in_range(diarization: Annotation, segment: Segment) -> Optional[str]:
    """
    åœ¨çµ¦å®šæ™‚é–“ç¯„åœå…§æ‰¾åˆ°èªªè©±æ™‚é–“æœ€é•·çš„èªªè©±äºº
    """
    speaker_durations = {}
    
    # éæ­· diarization ä¸­æ‰€æœ‰èˆ‡æ­¤æ®µè½é‡ç–Šçš„éƒ¨åˆ†
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        # è¨ˆç®—é‡ç–Šéƒ¨åˆ†
        overlap = segment & turn
        if overlap:
            duration = overlap.duration
            if speaker in speaker_durations:
                speaker_durations[speaker] += duration
            else:
                speaker_durations[speaker] = duration
    
    if not speaker_durations:
        return None
    
    # å›å‚³èªªè©±æ™‚é–“æœ€é•·çš„èªªè©±äºº
    return max(speaker_durations.items(), key=lambda x: x[1])[0]


def extract_embeddings_for_hybrid_segments(
    segments: List[Dict],
    audio_path: str,
    embedding_model,
    device: torch.device
) -> List[Dict]:
    """ç‚ºæ··åˆåˆ†æ®µæå– speaker embedding"""
    
    segments_with_embeddings = []
    
    if not segments:
        return []

    try:
        print(f"   ğŸ”Š Loading audio file for embedding: {audio_path}")
        waveform, sr = librosa.load(audio_path, sr=None)
    except Exception as e:
        print(f"   âŒ Error: Could not load audio file {audio_path}. Error: {e}")
        for segment in segments:
            segment['embedding'] = np.zeros(512)
            segment['embedding_extracted'] = False
            segments_with_embeddings.append(segment)
        return segments_with_embeddings

    print(f"   Extracting embeddings for {len(segments)} hybrid segments...")
    for segment in tqdm(segments, desc="   Extracting embeddings", unit="seg", ncols=80):
        start, end = segment['start'], segment['end']
        
        start_sample = int(start * sr)
        end_sample = int(end * sr)
        segment_waveform = waveform[start_sample:end_sample]

        embedding_extracted = False
        final_embedding = np.zeros(512)

        if segment_waveform.size > sr * 0.3:  # éœ€è¦è‡³å°‘ 0.3s çš„éŸ³é »
            try:
                segment_tensor = torch.from_numpy(segment_waveform).unsqueeze(0).to(device)
                with torch.no_grad():
                    embedding_output = embedding_model(segment_tensor)
                
                if hasattr(embedding_output, 'cpu'):
                    final_embedding = embedding_output.cpu().numpy()[0].flatten()
                else:
                    final_embedding = np.array(embedding_output)[0].flatten()
                embedding_extracted = True

            except Exception as e:
                print(f"     âŒ Error extracting embedding for segment [{start:.1f}-{end:.1f}s]: {e}")
                embedding_extracted = False
        else:
            embedding_extracted = False

        segment['embedding'] = final_embedding
        segment['embedding_extracted'] = embedding_extracted
        segments_with_embeddings.append(segment)
    
    return segments_with_embeddings


def merge_segments_by_speaker_and_similarity(
    segments: List[Dict],
    similarity_threshold: float,
    max_duration: float
) -> List[Dict]:
    """
    åŸºæ–¼ diarization èªªè©±äººæ¨™ç±¤å’Œ embedding ç›¸ä¼¼åº¦åˆä½µç›¸é„°ç‰‡æ®µ
    
    åˆä½µæ¢ä»¶ï¼š
    1. ç›¸é„°ç‰‡æ®µ
    2. ç›¸åŒçš„ diarization èªªè©±äººæ¨™ç±¤ï¼ˆå¦‚æœæœ‰ï¼‰
    3. Embedding ç›¸ä¼¼åº¦é«˜æ–¼é–¾å€¼
    4. åˆä½µå¾Œé•·åº¦ä¸è¶…éæœ€å¤§é™åˆ¶
    """
    
    if len(segments) <= 1:
        return segments
    
    merged_segments = []
    current_segment = segments[0].copy()
    merge_count = 0
    
    print(f"   ğŸ” é–‹å§‹æ··åˆåˆ†æ®µåˆä½µ (ç›¸ä¼¼åº¦é–¾å€¼: {similarity_threshold})")
    
    for i in range(1, len(segments)):
        next_segment = segments[i]
        
        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åˆä½µ
        can_merge = False
        similarity = 0.0
        
        # æ¢ä»¶ 1: diarization èªªè©±äººæ¨™ç±¤ç›¸åŒï¼ˆå¦‚æœæœ‰ï¼‰
        speaker_match = (
            current_segment.get('diarization_speaker') == next_segment.get('diarization_speaker')
            and current_segment.get('diarization_speaker') is not None
        )
        
        # æ¢ä»¶ 2: embedding ç›¸ä¼¼åº¦
        curr_has_embedding = current_segment.get('embedding_extracted', False)
        next_has_embedding = next_segment.get('embedding_extracted', False)
        
        embedding_similar = False
        if curr_has_embedding and next_has_embedding:
            emb1 = current_segment['embedding']
            emb2 = next_segment['embedding']
            
            try:
                similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                embedding_similar = similarity > similarity_threshold
            except Exception as e:
                print(f"     âš ï¸ ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
                embedding_similar = False
        
        # æ¢ä»¶ 3: æ™‚é–“é•·åº¦é™åˆ¶
        merged_duration = next_segment['end'] - current_segment['start']
        duration_ok = merged_duration <= max_duration
        
        # åˆä½µæ±ºç­–ï¼šèªªè©±äººæ¨™ç±¤ç›¸åŒ AND embedding ç›¸ä¼¼ AND é•·åº¦åˆè¦
        if speaker_match and embedding_similar and duration_ok:
            can_merge = True
        elif not speaker_match and embedding_similar and duration_ok:
            # å¦‚æœæ²’æœ‰èªªè©±äººæ¨™ç±¤è³‡è¨Šï¼Œåªé  embedding
            can_merge = True
        
        # é¡¯ç¤ºåˆä½µæ±ºç­–è³‡è¨Š
        speaker_info = f"diar_speaker: {current_segment.get('diarization_speaker', 'N/A')} vs {next_segment.get('diarization_speaker', 'N/A')}"
        similarity_info = f"embedding: {similarity:.3f} (>={similarity_threshold:.2f})" if curr_has_embedding and next_has_embedding else "embedding: N/A"
        
        print(f"     ğŸ” æ¯”è¼ƒæ®µè½ {current_segment['original_index']+1} vs {next_segment['original_index']+1}: "
              f"{speaker_info}, {similarity_info}, "
              f"åˆä½µé•·åº¦={merged_duration:.1f}s â†’ {'âœ…åˆä½µ' if can_merge else 'âŒåˆ†é–‹'}")
        
        if can_merge:
            # åˆä½µåˆ° current_segment
            current_segment['end'] = next_segment['end']
            current_segment['duration'] = current_segment['end'] - current_segment['start']
            current_segment['text'] += " " + next_segment['text']
            
            # ä½¿ç”¨å¹³å‡ embedding
            if (curr_has_embedding and next_has_embedding):
                current_segment['embedding'] = (current_segment['embedding'] + next_segment['embedding']) / 2
            
            merge_count += 1
            print(f"     âœ… åˆä½µæˆåŠŸ: [{current_segment['start']:.1f}-{current_segment['end']:.1f}s] '{current_segment['text'][:50]}...'")
        else:
            # ç„¡æ³•åˆä½µï¼Œä¿å­˜ current ä¸¦é–‹å§‹æ–°çš„
            merged_segments.append(current_segment)
            current_segment = next_segment.copy()
    
    # æ·»åŠ æœ€å¾Œä¸€å€‹æ®µè½
    merged_segments.append(current_segment)
    
    print(f"   ğŸ“Š æ··åˆåˆä½µçµ±è¨ˆ: åŸ·è¡Œäº† {merge_count} æ¬¡åˆä½µ,æœ€çµ‚ {len(merged_segments)} å€‹æ®µè½")
    return merged_segments


def assign_global_speaker_ids_hybrid(
    segments: List[Dict],
    db,
    episode_num: int,
    min_duration: float,
    max_duration: float,
    similarity_threshold: float
) -> Tuple[List[Tuple[float, float, int]], Dict[str, int]]:
    """ç‚ºæ··åˆåˆ†æ®µçš„æ¯å€‹æ®µè½åˆ†é… global speaker ID ä¸¦éæ¿¾é•·åº¦"""
    
    valid_segments = []
    local_to_global_map = {}
    
    print(f"   ğŸ¯ åˆ†é… Global Speaker IDs (æ··åˆæ¨¡å¼ï¼Œç›¸ä¼¼åº¦é–¾å€¼: {similarity_threshold})")
    
    for i, segment in enumerate(segments):
        start, end = segment['start'], segment['end']
        duration = end - start
        diar_speaker = segment.get('diarization_speaker', 'unknown')
        
        print(f"     è™•ç†æ®µè½ #{i+1}: [{start:.1f}-{end:.1f}s] ({duration:.1f}s) diarization={diar_speaker}")
        
        # éæ¿¾é•·åº¦
        if not (min_duration <= duration <= max_duration):
            print(f"       âŒ é•·åº¦ä¸åˆæ ¼ (éœ€è¦ {min_duration}-{max_duration}s)")
            continue
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆ embedding
        if not segment.get('embedding_extracted', False):
            print(f"       âŒ ç„¡æœ‰æ•ˆ embedding")
            continue
        
        embedding = segment['embedding']
        local_label = f"hybrid_seg_{i}_{diar_speaker}"
        
        # åœ¨è³‡æ–™åº«ä¸­æŸ¥æ‰¾æˆ–è¨»å†Š speaker
        try:
            print(f"       ğŸ” å‘¼å« find_similar_speakerï¼Œé–¾å€¼={similarity_threshold}")
            
            # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨embeddingæ›´æ–°
            update_embeddings = os.getenv("UPDATE_SPEAKER_EMBEDDINGS", "false").lower() == "true"
            update_weight = float(os.getenv("EMBEDDING_UPDATE_WEIGHT", "1.0"))
            
            speaker_id, similarity = db.find_similar_speaker(embedding, similarity_threshold,
                                                           update_embeddings, update_weight)
            print(f"       ğŸ” find_similar_speaker è¿”å›: speaker_id={speaker_id}, similarity={similarity}")
            
            if speaker_id is not None:
                print(f"       ğŸ” åŒ¹é…ç¾æœ‰ Global Speaker: {speaker_id} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                db.update_speaker_episode(speaker_id, episode_num, local_label, 1)
            else:
                speaker_id = db.add_speaker(embedding, episode_num, local_label, 1)
                print(f"       âœ¨ è¨»å†Šæ–° Global Speaker: {speaker_id}")
            
            # è¨˜éŒ„ mapping ä¸¦æ·»åŠ åˆ°çµæœ
            local_to_global_map[local_label] = speaker_id
            valid_segments.append((start, end, speaker_id))
            
            print(f"       âœ… æ®µè½å·²ä¿å­˜: Global ID {speaker_id}")
            
        except Exception as e:
            print(f"       âŒ Speaker ID åˆ†é…å¤±æ•—: {e}")
            continue
    
    return valid_segments, local_to_global_map