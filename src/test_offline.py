#!/usr/bin/env python3
"""
å®Œå…¨é›¢ç·šæ¨¡å‹æ¸¬è©¦è…³æœ¬ï¼ˆæœ€çµ‚ç‰ˆ - å…¨æ‰‹å‹•ï¼‰
"""
import os
from pathlib import Path
import torch
import yaml
from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.audio.models.segmentation import PyanNet
from pyannote.audio.models.embedding import WeSpeakerResNet34
from pyannote.audio.tasks import SpeakerDiarization as SpeakerDiarizationTask

class DummyProtocol:
    def __init__(self):
        self.preprocessors = {}
        self.scope = "file"  # å¿…éœ€çš„ scope å±¬æ€§
    
    @property
    def name(self):
        return "dummy"
    
    def train(self):
        # è‡³å°‘è¦æœ‰ä¸€å€‹å…ƒç´ ï¼Œé¿å… StopIteration
        yield {"uri": "dummy", "audio": "dummy.wav"}
    
    def development(self):
        yield {"uri": "dummy", "audio": "dummy.wav"}
    
    def test(self):
        yield {"uri": "dummy", "audio": "dummy.wav"}

def load_model_manually(model_class, config_path, checkpoint_path, device="cpu", attach_task=False):
    """
    ä¸€å€‹è¼”åŠ©å‡½æ•¸ï¼Œç”¨æ–¼æ‰‹å‹•å¯¦ä¾‹åŒ–æ¨¡å‹ã€è¼‰å…¥æ¬Šé‡ï¼Œä¸¦å¯é¸æ“‡æ€§åœ°é™„åŠ ä»»å‹™ã€‚
    """
    print(f"   - æ­£åœ¨æ‰‹å‹•è¼‰å…¥ {model_class.__name__}...")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    model_params = config.get('model', {})
    model_params.pop('_target_', None)
    
    model = model_class(**model_params)
    
    if attach_task:
        if 'task' in config:
            # ğŸ”¥ ç°¡åŒ–ä½†æœ‰æ•ˆçš„ task è¨­å®š
            from pyannote.audio.core.task import Specifications
            from pyannote.core import SlidingWindow
            import torch.nn as nn
            
            # å¾é…ç½®ç²å–åƒæ•¸
            task_config = config['task']
            duration = task_config.get('duration', 10.0)
            max_speakers = task_config.get('max_speakers_per_frame', 2)
            
            # å»ºç«‹ specifications
            step = 0.01  # 10ms step
            window = SlidingWindow(duration=duration, step=step)
            classes = [f"speaker_{i}" for i in range(max_speakers + 1)]
            
            specifications = Specifications(
                problem="multilabel_classification",
                resolution=window,
                duration=duration,
                classes=classes,
                permutation_invariant=True
            )
            
            model._specifications = specifications
            
            # ğŸ”¥ ç‚º PyanNet æ‰‹å‹•å»ºç«‹ç¼ºå¤±çš„å±¤
            if model_class.__name__ == 'PyanNet':
                # ç²å–æ¨¡å‹ç¶­åº¦
                linear_config = model_params.get('linear', {})
                linear_dim = linear_config.get('hidden_size', 128)
                num_classes = len(classes)
                
                # æ‰‹å‹•å»ºç«‹ classifier å±¤
                if not hasattr(model, 'classifier'):
                    model.classifier = nn.Linear(linear_dim, num_classes)
                
                # æ‰‹å‹•å»ºç«‹ activation å±¤
                if not hasattr(model, 'activation'):
                    model.activation = nn.Sigmoid()
                
                print(f"     ğŸ”§ å·²å»ºç«‹ classifier({linear_dim} -> {num_classes}) å’Œ activation å±¤")
    
    # PyTorch 2.6+ é è¨­ weights_only=Trueï¼Œé€™è£¡å¼·åˆ¶è¨­ç‚º False ä»¥æ”¯æ´èˆŠ checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # ğŸ”¥ ç‚º PyanNet æ ¹æ“šæª¢æŸ¥é»èª¿æ•´ classifier ç¶­åº¦
    if model_class.__name__ == 'PyanNet' and attach_task:
        if 'classifier.weight' in checkpoint['state_dict']:
            # å¾æª¢æŸ¥é»ç²å–æ­£ç¢ºçš„ç¶­åº¦
            checkpoint_classifier_shape = checkpoint['state_dict']['classifier.weight'].shape
            correct_num_classes = checkpoint_classifier_shape[0]  # è¼¸å‡ºç¶­åº¦
            correct_input_dim = checkpoint_classifier_shape[1]    # è¼¸å…¥ç¶­åº¦
            
            print(f"     ğŸ”§ å¾æª¢æŸ¥é»æª¢æ¸¬åˆ° classifier ç¶­åº¦: {correct_input_dim} -> {correct_num_classes}")
            
            # é‡æ–°å»ºç«‹æ­£ç¢ºç¶­åº¦çš„ classifier
            import torch.nn as nn
            model.classifier = nn.Linear(correct_input_dim, correct_num_classes)
            
            # æ›´æ–° specifications çš„é¡åˆ¥æ•¸
            if hasattr(model, '_specifications'):
                # å»ºç«‹æ­£ç¢ºæ•¸é‡çš„é¡åˆ¥
                classes = [f"speaker_{i}" for i in range(correct_num_classes)]
                from pyannote.audio.core.task import Specifications
                from pyannote.core import SlidingWindow
                
                duration = model._specifications.duration
                window = model._specifications.resolution
                
                model._specifications = Specifications(
                    problem="multilabel_classification",
                    resolution=window,
                    duration=duration,
                    classes=classes,
                    permutation_invariant=True
                )
    
    model.load_state_dict(checkpoint['state_dict'], strict=False)
    
    model.to(device)
    model.eval()
    print(f"     âœ… {model_class.__name__} è¼‰å…¥æˆåŠŸã€‚")
    return model

def test_offline_diarization_fully_manual():
    """
    æ¸¬è©¦å®Œå…¨é›¢ç·šçš„ Speaker Diarization Pipeline è¼‰å…¥ã€‚
    æ¡ç”¨æœ€åº•å±¤ã€æœ€å¯é çš„å…¨æ‰‹å‹•è¼‰å…¥æ–¹å¼ã€‚
    """
    print("="*50)
    print("ğŸš€ é–‹å§‹é€²è¡Œ Pyannote Audio é›¢ç·šè¼‰å…¥æ¸¬è©¦ (å…¨æ‰‹å‹•æ¨¡å¼) ğŸš€")
    print("="*50)

    script_dir = Path(__file__).parent.parent
    base_model_path = script_dir / "models" / "direct"
    
    seg_dir = base_model_path / "segmentation-3.0"
    emb_dir = base_model_path / "wespeaker-voxceleb-resnet34-LM"

    print("\nğŸ¤– 1. æ¸¬è©¦ PyTorch...")
    try:
        print(f"   âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"   âœ… ä½¿ç”¨è£ç½®: {device.type.upper()}")
    except Exception as e:
        print(f"   âŒ PyTorch æ¸¬è©¦å¤±æ•—: {e}")
        return

    print("\nğŸ”§ 2. æ‰‹å‹•è¼‰å…¥å­æ¨¡å‹...")
    try:
        # ğŸ”¥ ç‚º Segmentation æ¨¡å‹è¼‰å…¥ï¼ˆä¸é™„åŠ  Taskï¼‰
        segmentation_model = load_model_manually(
            PyanNet,
            seg_dir / "config.yaml",
            seg_dir / "pytorch_model.bin",
            device,
            attach_task=True
        )
        
        # ğŸ”¥ ç‚º Embedding æ¨¡å‹è¼‰å…¥
        embedding_model = load_model_manually(
            WeSpeakerResNet34,
            emb_dir / "config.yaml",
            emb_dir / "pytorch_model.bin",
            device
        )
        
    except Exception as e:
        print(f"   âŒ è¼‰å…¥å­æ¨¡å‹å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return

    # 3. å°‡é è¼‰å…¥çš„æ¨¡å‹æ³¨å…¥ Pipeline
    print("\nğŸ”„ 3. å°‡å­æ¨¡å‹æ³¨å…¥ SpeakerDiarization Pipeline...")
    try:
        # æ ¸å¿ƒæ­¥é©Ÿï¼šå°‡æ¨¡å‹ç‰©ä»¶ç›´æ¥å‚³å…¥å»ºæ§‹å­
        pipeline = SpeakerDiarization(
            segmentation=segmentation_model,
            embedding=embedding_model,
        )
        pipeline.to(device)
        print("   âœ… Pipeline å¯¦ä¾‹åŒ–ä¸¦æ³¨å…¥æ¨¡å‹æˆåŠŸï¼")

    except Exception as e:
        print(f"   âŒ Pipeline å¯¦ä¾‹åŒ–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return

    print("\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ‰€æœ‰æ¸¬è©¦é€šéï¼ŒPyannote Pipeline å·²åœ¨å®Œå…¨é›¢ç·šæ¨¡å¼ä¸‹æˆåŠŸå»ºç«‹ï¼ ğŸ‰ğŸ‰ğŸ‰")
    print("="*50)


if __name__ == "__main__":
    test_offline_diarization_fully_manual()
