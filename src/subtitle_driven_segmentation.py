#!/usr/bin/env python3
"""
å­—å¹•é©…å‹•çš„speakeråˆ†æ®µé‚è¼¯
- åŸºæ–¼å­—å¹•æ™‚é–“è»¸é åˆ‡åˆ†éŸ³é »
- ä½¿ç”¨embeddingæ¯”è¼ƒç›¸é„°ç‰‡æ®µæ±ºå®šæ˜¯å¦åˆä½µ
- ç¢ºä¿ä¸æœƒéºæ¼å­—å¹•å…§å®¹
"""

import numpy as np
from typing import List, Tuple, Optional, Dict
import torch
from tqdm import tqdm


def segment_by_subtitle_driven(
    subtitles: List[Tuple[float, str]],
    audio_path: str,
    embedding_model,
    device: torch.device,
    db,  # è³‡æ–™åº«ç‰©ä»¶
    episode_num: int,
    min_duration: float = 2.0,
    max_duration: float = 15.0,
    similarity_threshold: float = 0.85
) -> Tuple[List[Tuple[float, float, int]], Dict[str, int]]:
    """
    åŸºæ–¼å­—å¹•æ™‚é–“è»¸çš„speakeråˆ†æ®µï¼Œç„¶å¾Œç”¨embeddingåˆä½µ
    
    Args:
        subtitles: å­—å¹•åˆ—è¡¨ [(timestamp, text), ...]
        audio_path: éŸ³é »æª”æ¡ˆè·¯å¾‘
        embedding_model: speaker embeddingæ¨¡å‹
        device: è¨ˆç®—è¨­å‚™
        db: speakerè³‡æ–™åº«
        episode_num: é›†æ•¸
        min_duration: æœ€å°æ®µè½é•·åº¦
        max_duration: æœ€å¤§æ®µè½é•·åº¦
        similarity_threshold: speakerç›¸ä¼¼åº¦é–¾å€¼
        
    Returns:
        (segments_with_global_ids, local_to_global_mapping)
    """
    
    if not subtitles:
        print("   âŒ æ²’æœ‰å­—å¹•è³‡æ–™")
        return [], {}
    
    print(f"   ğŸ“š å­—å¹•é©…å‹•åˆ†æ®µæ¨¡å¼ï¼š{len(subtitles)} å¥å­—å¹•")
    
    # Step 1: å»ºç«‹åˆå§‹çš„å­—å¹•éŸ³é »ç‰‡æ®µ
    subtitle_segments = create_subtitle_segments(subtitles, audio_path)
    
    # Add audio_path to each segment for embedding extraction
    for segment in subtitle_segments:
        segment['audio_path'] = audio_path
    print(f"   âœ‚ï¸ å»ºç«‹äº† {len(subtitle_segments)} å€‹å­—å¹•éŸ³é »ç‰‡æ®µ")
    
    # Step 2: ç‚ºæ¯å€‹ç‰‡æ®µæå–embedding
    segments_with_embeddings = extract_embeddings_for_segments(
        subtitle_segments, embedding_model, device
    )
    print(f"   ğŸ§¬ æå–äº† {len(segments_with_embeddings)} å€‹embedding")
    
    # Step 3: åŸºæ–¼embeddingç›¸ä¼¼åº¦åˆä½µç›¸é„°ç‰‡æ®µ
    merged_segments = merge_segments_by_similarity(
        segments_with_embeddings, similarity_threshold, max_duration
    )
    print(f"   ğŸ”— åˆä½µå¾Œå‰©é¤˜ {len(merged_segments)} å€‹æ®µè½")
    
    # Step 4: éæ¿¾é•·åº¦ä¸¦åˆ†é…global speaker ID
    final_segments, local_to_global_map = assign_global_speaker_ids(
        merged_segments, db, episode_num, min_duration, max_duration, similarity_threshold
    )
    
    print(f"   âœ… æœ€çµ‚ç”¢ç”Ÿ {len(final_segments)} å€‹æœ‰æ•ˆæ®µè½")
    return final_segments, local_to_global_map


def create_subtitle_segments(
    subtitles: List[Tuple[float, str]], 
    audio_path: str
) -> List[Dict]:
    """
    æ ¹æ“šå­—å¹•æ™‚é–“è»¸å‰µå»ºéŸ³é »ç‰‡æ®µ
    
    æ¯å€‹å­—å¹•çš„æŒçºŒæ™‚é–“ = åˆ°ä¸‹ä¸€å€‹å­—å¹•é–‹å§‹çš„æ™‚é–“é–“éš”
    """
    import librosa
    
    # ç²å–éŸ³é »ç¸½é•·åº¦
    audio_duration = librosa.get_duration(path=audio_path)
    
    segments = []
    for i, (start_time, text) in enumerate(subtitles):
        # è¨ˆç®—é€™å¥å­—å¹•çš„çµæŸæ™‚é–“
        if i < len(subtitles) - 1:
            # ä¸æ˜¯æœ€å¾Œä¸€å¥ï¼šçµæŸæ™‚é–“ = ä¸‹ä¸€å¥é–‹å§‹æ™‚é–“
            end_time = subtitles[i + 1][0]
        else:
            # æœ€å¾Œä¸€å¥ï¼šä¼°ç®—æŒçºŒæ™‚é–“ï¼ˆæ¯å­—0.3ç§’ï¼Œæœ€å°‘2ç§’ï¼Œæœ€å¤š8ç§’ï¼‰
            estimated_duration = max(2.0, min(8.0, len(text) * 0.3))
            end_time = min(start_time + estimated_duration, audio_duration)
        
        # ç¢ºä¿æ™‚é–“ç¯„åœåˆç†
        if end_time > start_time + 0.5:  # è‡³å°‘0.5ç§’
            segments.append({
                'start': start_time,
                'end': end_time,
                'text': text,
                'original_index': i,
                'duration': end_time - start_time
            })
            print(f"     ğŸ“ #{i+1}: [{start_time:.1f}-{end_time:.1f}s] ({end_time-start_time:.1f}s) '{text}'")
        else:
            print(f"     âš ï¸ è·³ééçŸ­å­—å¹• #{i+1}: {text}")
    
    return segments


def extract_embeddings_for_segments(
    segments: List[Dict],
    embedding_model,
    device: torch.device
) -> List[Dict]:
    """ç‚ºæ¯å€‹å­—å¹•éŸ³é »ç‰‡æ®µæå–speaker embedding"""
    
    segments_with_embeddings = []
    
    for i, segment in enumerate(tqdm(segments, desc="   æå–embeddings", unit="seg", ncols=80)):
        start, end, text = segment['start'], segment['end'], segment['text']
        
        # å‰µå»ºsegment dictçµ¦pyannote
        segment_dict = {
            "audio": segment['audio_path'],
            "start": start,
            "end": end
        }
        
        try:
            with torch.no_grad():
                embedding_output = embedding_model(segment_dict)
            
            # è™•ç†ä¸åŒé¡å‹çš„embeddingè¼¸å‡º
            if hasattr(embedding_output, 'cpu'):
                embedding_np = embedding_output.cpu().numpy()
            elif isinstance(embedding_output, dict):
                if 'embeddings' in embedding_output:
                    embedding_np = embedding_output['embeddings']
                elif 'embedding' in embedding_output:
                    embedding_np = embedding_output['embedding']
                else:
                    embedding_np = list(embedding_output.values())[0]
                
                # è½‰æ›ç‚ºnumpy
                if hasattr(embedding_np, 'cpu'):
                    embedding_np = embedding_np.cpu().numpy()
            else:
                embedding_np = embedding_output
            
            # ç¢ºä¿æ˜¯1D embedding
            if embedding_np.ndim > 1:
                embedding_np = embedding_np.flatten()
            
            # è¤‡è£½segmentè³‡æ–™ä¸¦åŠ å…¥embedding
            segment_copy = segment.copy()
            segment_copy['embedding'] = embedding_np
            segment_copy['embedding_extracted'] = True
            segments_with_embeddings.append(segment_copy)
            
            print(f"     âœ… #{i+1}: embedding shape {embedding_np.shape}, mean={np.mean(embedding_np):.4f}")
            
        except Exception as e:
            print(f"     âŒ #{i+1}: embeddingæå–å¤±æ•—: {e}")
            # æ·»åŠ ç©ºembedding
            segment_copy = segment.copy()
            segment_copy['embedding'] = np.zeros(512)
            segment_copy['embedding_extracted'] = False
            segments_with_embeddings.append(segment_copy)
    
    return segments_with_embeddings


def merge_segments_by_similarity(
    segments: List[Dict],
    similarity_threshold: float,
    max_duration: float
) -> List[Dict]:
    """åŸºæ–¼embeddingç›¸ä¼¼åº¦åˆä½µç›¸é„°çš„éŸ³é »ç‰‡æ®µ"""
    
    if len(segments) <= 1:
        return segments
    
    merged_segments = []
    current_segment = segments[0].copy()
    merge_count = 0
    
    print(f"   ğŸ” é–‹å§‹åŸºæ–¼ç›¸ä¼¼åº¦åˆä½µ (é–¾å€¼: {similarity_threshold})")
    
    for i in range(1, len(segments)):
        next_segment = segments[i]
        
        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åˆä½µ
        can_merge = False
        similarity = 0.0
        
        # éœ€è¦å…©å€‹segmentéƒ½æœ‰æœ‰æ•ˆçš„embedding
        if (current_segment.get('embedding_extracted', False) and 
            next_segment.get('embedding_extracted', False)):
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            emb1 = current_segment['embedding']
            emb2 = next_segment['embedding']
            
            try:
                similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                
                # æª¢æŸ¥åˆä½µæ¢ä»¶
                merged_duration = next_segment['end'] - current_segment['start']
                
                can_merge = (
                    similarity > similarity_threshold and 
                    merged_duration <= max_duration
                )
                
                print(f"     ğŸ” æ¯”è¼ƒæ®µè½ {current_segment['original_index']+1} vs {next_segment['original_index']+1}: "
                      f"ç›¸ä¼¼åº¦={similarity:.3f}, åˆä½µé•·åº¦={merged_duration:.1f}s "
                      f"â†’ {'âœ…åˆä½µ' if can_merge else 'âŒåˆ†é–‹'}")
                
            except Exception as e:
                print(f"     âš ï¸ ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
                can_merge = False
        
        if can_merge:
            # åˆä½µåˆ°current_segment
            current_segment['end'] = next_segment['end']
            current_segment['duration'] = current_segment['end'] - current_segment['start']
            current_segment['text'] += " " + next_segment['text']
            
            # ä½¿ç”¨å¹³å‡embedding
            if ('embedding' in current_segment and 'embedding' in next_segment and
                current_segment.get('embedding_extracted', False) and 
                next_segment.get('embedding_extracted', False)):
                current_segment['embedding'] = (current_segment['embedding'] + next_segment['embedding']) / 2
            
            merge_count += 1
            print(f"     âœ… åˆä½µæˆåŠŸ: [{current_segment['start']:.1f}-{current_segment['end']:.1f}s] '{current_segment['text'][:50]}...'")
        else:
            # ç„¡æ³•åˆä½µï¼Œä¿å­˜currentä¸¦é–‹å§‹æ–°çš„
            merged_segments.append(current_segment)
            current_segment = next_segment.copy()
    
    # æ·»åŠ æœ€å¾Œä¸€å€‹æ®µè½
    merged_segments.append(current_segment)
    
    print(f"   ğŸ“Š åˆä½µçµ±è¨ˆ: åŸ·è¡Œäº† {merge_count} æ¬¡åˆä½µï¼Œæœ€çµ‚ {len(merged_segments)} å€‹æ®µè½")
    return merged_segments


def assign_global_speaker_ids(
    segments: List[Dict],
    db,
    episode_num: int,
    min_duration: float,
    max_duration: float,
    similarity_threshold: float
) -> Tuple[List[Tuple[float, float, int]], Dict[str, int]]:
    """ç‚ºæ¯å€‹æ®µè½åˆ†é…global speaker IDä¸¦éæ¿¾é•·åº¦"""
    
    valid_segments = []
    local_to_global_map = {}
    
    print(f"   ğŸ¯ åˆ†é…Global Speaker IDs (ç›¸ä¼¼åº¦é–¾å€¼: {similarity_threshold})")
    
    for i, segment in enumerate(segments):
        start, end = segment['start'], segment['end']
        duration = end - start
        
        print(f"     è™•ç†æ®µè½ #{i+1}: [{start:.1f}-{end:.1f}s] ({duration:.1f}s)")
        
        # éæ¿¾é•·åº¦
        if not (min_duration <= duration <= max_duration):
            print(f"       âŒ é•·åº¦ä¸åˆæ ¼ (éœ€è¦ {min_duration}-{max_duration}s)")
            continue
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆembedding
        if not segment.get('embedding_extracted', False):
            print(f"       âŒ ç„¡æœ‰æ•ˆembedding")
            continue
        
        embedding = segment['embedding']
        local_label = f"subtitle_seg_{i}"
        
        # åœ¨è³‡æ–™åº«ä¸­æŸ¥æ‰¾æˆ–è¨»å†Šspeaker
        try:
            speaker_id, similarity = db.find_similar_speaker(embedding, similarity_threshold)
            
            if speaker_id is not None:
                print(f"       ğŸ” åŒ¹é…ç¾æœ‰Global Speaker: {speaker_id} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                db.update_speaker_episode(speaker_id, episode_num, local_label, 1)
            else:
                speaker_id = db.add_speaker(embedding, episode_num, local_label, 1)
                print(f"       âœ¨ è¨»å†Šæ–°Global Speaker: {speaker_id}")
            
            # è¨˜éŒ„mappingä¸¦æ·»åŠ åˆ°çµæœ
            local_to_global_map[local_label] = speaker_id
            valid_segments.append((start, end, speaker_id))
            
            print(f"       âœ… æ®µè½å·²ä¿å­˜: Global ID {speaker_id}")
            
        except Exception as e:
            print(f"       âŒ Speaker IDåˆ†é…å¤±æ•—: {e}")
            continue
    
    return valid_segments, local_to_global_map


def compute_segment_embedding(
    audio_path: str,
    start: float,
    end: float,
    embedding_model,
    device: torch.device
) -> Optional[np.ndarray]:
    """ç‚ºå–®å€‹éŸ³é »ç‰‡æ®µè¨ˆç®—embedding"""
    
    segment_dict = {
        "audio": audio_path,
        "start": start,
        "end": end
    }
    
    try:
        with torch.no_grad():
            embedding_output = embedding_model(segment_dict)
        
        # è™•ç†ä¸åŒé¡å‹çš„è¼¸å‡º
        if hasattr(embedding_output, 'cpu'):
            embedding_np = embedding_output.cpu().numpy()
        elif isinstance(embedding_output, dict):
            if 'embeddings' in embedding_output:
                embedding_np = embedding_output['embeddings']
            elif 'embedding' in embedding_output:
                embedding_np = embedding_output['embedding']
            else:
                embedding_np = list(embedding_output.values())[0]
            
            if hasattr(embedding_np, 'cpu'):
                embedding_np = embedding_np.cpu().numpy()
        else:
            embedding_np = embedding_output
        
        # ç¢ºä¿æ˜¯1D
        if embedding_np.ndim > 1:
            embedding_np = embedding_np.flatten()
        
        return embedding_np
        
    except Exception as e:
        print(f"     âš ï¸ Embeddingè¨ˆç®—å¤±æ•—: {e}")
        return None