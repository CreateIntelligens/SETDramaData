#!/usr/bin/env python3
"""
Pyannote Speaker Segmentation with Global Speaker Database
èªªè©±äººç´šåˆ¥åˆ†æ®µç³»çµ±ï¼š
- åˆä½µåŒèªªè©±äººç‰‡æ®µï¼Œæå–ç©©å®šçš„è²ç´‹ç‰¹å¾µ
- æä¾›æº–ç¢ºçš„è·¨é›†èªªè©±äººè­˜åˆ¥
"""

import os
import sys
import argparse
from pathlib import Path

# ğŸ¯ ç«‹å³è¨­å®š HuggingFace é›¢ç·šæ¨¡å¼ï¼ˆåœ¨ä»»ä½• import ä¹‹å‰ï¼‰
script_dir = Path(__file__).parent.parent
models_dir = script_dir / "models"

# å°‹æ‰¾æ­£ç¢ºçš„ HF å¿«å–è·¯å¾‘
hf_cache_dir = None
hf_cache_options = [
    models_dir / "huggingface",
    models_dir / "huggingface" / "hub"
]

for cache_path in hf_cache_options:
    if cache_path.exists():
        speaker_diar_path = cache_path / "models--pyannote--speaker-diarization-3.1"
        if speaker_diar_path.exists():
            hf_cache_dir = cache_path
            break

# è¨­å®šç’°å¢ƒè®Šæ•¸ï¼ˆåœ¨ import torch ä¹‹å‰ï¼‰
if hf_cache_dir:
    os.environ.update({
        'HF_HOME': str(hf_cache_dir.parent) if hf_cache_dir.name == 'hub' else str(hf_cache_dir),
        'HF_HUB_CACHE': str(hf_cache_dir),
        'TRANSFORMERS_CACHE': str(hf_cache_dir.parent) if hf_cache_dir.name == 'hub' else str(hf_cache_dir),
        'TRANSFORMERS_OFFLINE': '1',
        'HF_DATASETS_OFFLINE': '1',
        'HUGGINGFACE_HUB_OFFLINE': '1'
    })

# è‡ªå‹•æª¢æ¸¬ç³»çµ±é…ç½®ä¸¦æ‡‰ç”¨æœ€ä½³è¨­å®š
# åŠ è¼‰æª¢æ¸¬æ¨¡çµ„å‰ï¼Œå…ˆè¨­å®šåŸºæœ¬ç’°å¢ƒè®Šæ•¸
os.environ.update({
    'MKL_SERVICE_FORCE_INTEL': '1',
    'MKL_THREADING_LAYER': 'GNU',
    'OMP_NUM_THREADS': '1',
    'MKL_NUM_THREADS': '1',
    'MKL_DEBUG_CPU_TYPE': '5',
    'KMP_DUPLICATE_LIB_OK': 'TRUE',
    'KMP_WARNINGS': 'FALSE',
    'TORCH_WARN': '0',
    'PYTORCH_DISABLE_WARNINGS': '1'
})

import torch
import numpy as np
from pathlib import Path

# é¡¯ç¤º HuggingFace å¿«å–è¨­å®šç‹€æ…‹
if hf_cache_dir:
    print(f"ğŸ”§ ä½¿ç”¨ HuggingFace å¿«å–: {hf_cache_dir}")
    print(f"   é›¢ç·šæ¨¡å¼: å·²å•Ÿç”¨")
else:
    print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ HuggingFace å¿«å–ç›®éŒ„")
    
    # æ­£ç¢ºå•Ÿç”¨é›¢ç·šæ¨¡å¼
    try:
        from huggingface_hub import set_offline_mode
        set_offline_mode()
        print("   âœ… å·²å•Ÿç”¨ HuggingFace Hub é›¢ç·šæ¨¡å¼")
    except ImportError:
        print("   âš ï¸ ç„¡æ³•å°å…¥ huggingface_hub.set_offline_mode")
    except Exception as e:
        print(f"   âš ï¸ è¨­å®šé›¢ç·šæ¨¡å¼å¤±æ•—: {e}")
from typing import List, Tuple, Dict
import librosa
import soundfile as sf
from tqdm import tqdm
import warnings

# éœéŸ³å„ç¨®ç…©äººçš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*NNPACK.*")
warnings.filterwarnings("ignore", message=".*Could not initialize NNPACK.*")
warnings.filterwarnings("ignore", message=".*TensorFloat-32.*")

# è¨­å®š logging ç­‰ç´šé¿å… C++ å±¤é¢çš„è­¦å‘Š
import logging
logging.getLogger("torch").setLevel(logging.ERROR)

# è¨­å®š PyTorch ç·šç¨‹æ•¸ä»¥é¿å… MKL è¡çªï¼ˆæ›´ä¿å®ˆï¼‰
torch.set_num_threads(1)
torch.set_num_interop_threads(1)

# ğŸš¨ æ¿€é€²è¨˜æ†¶é«”æ§åˆ¶ï¼ˆé‡å° PyTorch 2.6ï¼‰
torch.backends.cudnn.benchmark = False  # ç¦ç”¨ cudnn benchmark
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.enabled = False    # å®Œå…¨ç¦ç”¨ cudnnï¼ˆæœ€ä¿å®ˆï¼‰

# PyTorch 2.x è¨˜æ†¶é«”å„ªåŒ–
if hasattr(torch, 'set_float32_matmul_precision'):
    torch.set_float32_matmul_precision('medium')
if hasattr(torch, '_set_print_file'):
    torch.set_warn_always(False)
    
# å¼·åˆ¶è¨­å®šè¨˜æ†¶é«”åˆ†é…ç­–ç•¥ï¼ˆPyTorch 2.6 ç›¸å®¹ï¼‰
os.environ.update({
    'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:64,expandable_segments:False,roundup_power2_divisions:1',
    'TORCH_CUDNN_V8_API_ENABLED': '0',  # ç¦ç”¨ CUDNN v8 API
    'CUDA_LAUNCH_BLOCKING': '1'         # åŒæ­¥ CUDA åŸ·è¡Œ
})

# ç°¡åŒ–é…ç½®ï¼ˆåƒè€ƒèƒ½ç”¨çš„å°ˆæ¡ˆï¼‰
# ç¦ç”¨å•é¡Œå¾Œç«¯
if hasattr(torch.backends, 'mkldnn'):
    torch.backends.mkldnn.enabled = False
if hasattr(torch.backends, 'mkl'):
    torch.backends.mkl.enabled = False

# æª¢æŸ¥ GPU å¯ç”¨æ€§ä¸¦å„ªåŒ–è¨˜æ†¶é«”è¨­å®š
if torch.cuda.is_available():
    print(f"âœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
    
    # ğŸš¨ æ¥µåº¦ä¿å®ˆçš„ CUDA è¨˜æ†¶é«”æ§åˆ¶ï¼ˆPyTorch 2.6ï¼‰
    torch.cuda.empty_cache()
    torch.cuda.memory.set_per_process_memory_fraction(0.3)  # åªç”¨ 30% GPU è¨˜æ†¶é«”
    
    # è¨­å®š CUDA åŒæ­¥æ¨¡å¼ï¼ˆé¿å…è¨˜æ†¶é«”ç«¶çˆ­ï¼‰
    torch.cuda.synchronize()
    
    # PyTorch 2.6 è¨˜æ†¶é«”ç­–ç•¥
    if hasattr(torch.cuda, 'set_memory_allocator'):
        try:
            torch.cuda.set_memory_allocator('native')
        except:
            pass
    
    # é¡¯ç¤º GPU è¨˜æ†¶é«”è³‡è¨Š
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"   ğŸ“Š GPU è¨˜æ†¶é«”: {gpu_memory:.1f}GB (é™åˆ¶ä½¿ç”¨ 30%)")
    
else:
    print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")

# Python è¨˜æ†¶é«”å„ªåŒ–
import gc
gc.collect()  # å¼·åˆ¶åƒåœ¾å›æ”¶

# æ·»åŠ  src ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from speaker_database import SpeakerDatabase
from speaker_level_segmentation import segment_by_speaker_level_approach

# Pyannote imports
try:
    from pyannote.audio import Pipeline
    from pyannote.audio.pipelines import SpeakerDiarization
    from pyannote.audio.pipelines.utils.hook import ProgressHook
    from pyannote.core import Annotation
    import torch.nn.functional as F
    import yaml
except ImportError as e:
    print(f"âŒ Error importing pyannote.audio: {e}")
    print("Please install pyannote.audio: pip install pyannote.audio")
    sys.exit(1)



class EmbeddingInference:
    """Embedding model wrapper for speaker verification"""
    
    def __init__(self, device: torch.device):
        self.device = device
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """Load the embedding model"""
        try:
            from pyannote.audio import Model
            
            # å˜—è©¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾‘ - æ›´æ™ºèƒ½çš„è·¯å¾‘æª¢æ¸¬
            current_dir = Path.cwd()
            
            # æª¢æ¸¬å°ˆæ¡ˆæ ¹ç›®éŒ„
            if current_dir.name == 'src':
                project_root = current_dir.parent
            elif (current_dir / 'src').exists():
                project_root = current_dir
            else:
                project_root = Path(__file__).parent.parent
                
            # ä½¿ç”¨ HuggingFace å¿«å–æ–¹å¼è¼‰å…¥ embedding æ¨¡å‹
            print(f"   ğŸ“ è¼‰å…¥ embedding æ¨¡å‹ï¼ˆä½¿ç”¨ HF å¿«å–ï¼‰...")
            
            try:
                # ç›´æ¥ä½¿ç”¨ repo IDï¼Œä¾è³´ HF å¿«å–
                self.model = Model.from_pretrained(
                    "pyannote/wespeaker-voxceleb-resnet34-LM",
                    use_auth_token=None
                ).to(self.device)
                self.model.eval()
                print("   âœ… Embedding model loaded successfullyï¼ˆHF å¿«å–æ–¹å¼ï¼‰")
                
            except Exception as e:
                print(f"   âŒ å¿«å–è¼‰å…¥å¤±æ•—: {e}")
                print("   ğŸ’¡ è«‹ç¢ºèªå·²åŸ·è¡Œ setup_hf_cache.py è¨­å®šå¿«å–")
                raise Exception("è«‹å…ˆåŸ·è¡Œ setup_hf_cache.py å»ºç«‹ HuggingFace å¿«å–çµæ§‹")
        except Exception as e:
            print(f"   âŒ Error loading embedding model: {e}")
            raise


def load_subtitles(subtitle_file: str, fps: float = 30.0) -> List[Tuple[float, str]]:
    """Load subtitles from file, supporting both HH:MM:SS:FF and seconds format"""
    subtitles = []
    
    def parse_timecode(timecode_str: str) -> float:
        """Parse timecode in HH:MM:SS:FF format to seconds"""
        try:
            # ç§»é™¤ BOM å­—å…ƒ
            timecode_str = timecode_str.lstrip('\ufeff')
            
            # å˜—è©¦ç›´æ¥è§£æç‚ºæµ®é»æ•¸ï¼ˆç§’æ ¼å¼ï¼‰
            try:
                return float(timecode_str)
            except ValueError:
                pass
            
            # è§£æ HH:MM:SS:FF æ ¼å¼
            if ':' in timecode_str:
                parts = timecode_str.split(':')
                if len(parts) == 4:  # HH:MM:SS:FF
                    hours, minutes, seconds, frames = map(int, parts)
                    total_seconds = hours * 3600 + minutes * 60 + seconds + frames / fps
                    return total_seconds
                elif len(parts) == 3:  # MM:SS:FF or HH:MM:SS
                    if int(parts[0]) > 59:  # å¯èƒ½æ˜¯ HH:MM:SS
                        hours, minutes, seconds = map(int, parts)
                        return hours * 3600 + minutes * 60 + seconds
                    else:  # MM:SS:FF
                        minutes, seconds, frames = map(int, parts)
                        return minutes * 60 + seconds + frames / fps
            
            raise ValueError(f"Unsupported timecode format: {timecode_str}")
            
        except Exception as e:
            raise ValueError(f"Could not parse timecode '{timecode_str}': {e}")
    
    try:
        with open(subtitle_file, 'r', encoding='utf-8-sig') as f:  # utf-8-sig è‡ªå‹•è™•ç† BOM
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    # è§£ææ ¼å¼ï¼štimecode text
                    parts = line.split(' ', 1)
                    if len(parts) >= 1:
                        timecode_str = parts[0]
                        text = parts[1] if len(parts) > 1 else ""
                        
                        # è§£ææ™‚é–“ç¢¼
                        timestamp = parse_timecode(timecode_str)
                        subtitles.append((timestamp, text))
                    else:
                        print(f"   âš ï¸ Warning: Invalid format at line {line_num}: {line}")
                except ValueError as e:
                    print(f"   âš ï¸ Warning: Could not parse line {line_num}: '{line}' - parts: {parts} ({e})")
                    continue
        
        print(f"   âœ… Loaded {len(subtitles)} subtitle entries")
        if subtitles:
            print(f"   ğŸ“Š Time range: {subtitles[0][0]:.2f}s - {subtitles[-1][0]:.2f}s")
        return subtitles
        
    except Exception as e:
        print(f"   âŒ Error loading subtitles: {e}")
        return []


def perform_speaker_diarization(audio_file: str, pipeline: SpeakerDiarization, device: torch.device) -> Annotation:
    """Perform speaker diarization using pyannote"""
    
    print(f"   Processing audio file: {audio_file}")
    
    try:
        # æª¢æŸ¥pipelineæ˜¯å¦å·²å¯¦ä¾‹åŒ–
        has_basic_components = hasattr(pipeline, '_segmentation') and hasattr(pipeline, '_embedding')
        has_instantiated = hasattr(pipeline, '_instantiated') and isinstance(pipeline._instantiated, dict)
        
        print(f"   ğŸ” Pipeline ç‹€æ…‹æª¢æŸ¥:")
        print(f"      åŸºæœ¬çµ„ä»¶: {has_basic_components}")  
        print(f"      å¯¦ä¾‹åŒ–ç‹€æ…‹: {has_instantiated}")
        
        if not (has_basic_components and has_instantiated):
            print("   âš ï¸ Pipeline æœªå®Œå…¨å¯¦ä¾‹åŒ–ï¼Œå˜—è©¦ä¿®å¾©...")
            try:
                from collections import OrderedDict
                from pyannote.audio.pipelines.clustering import AgglomerativeClustering
                
                # ç¢ºä¿ clustering çµ„ä»¶å­˜åœ¨
                if not hasattr(pipeline, '_clustering') or pipeline._clustering is None:
                    pipeline._clustering = AgglomerativeClustering()
                
                # è¨­å®šå¯¦ä¾‹åŒ–ç‹€æ…‹
                if not has_instantiated:
                    pipeline._instantiated = OrderedDict()
                    pipeline._instantiated["segmentation"] = {
                        "min_duration_off": 0.0,
                        "onset": 0.5,
                        "offset": 0.5
                    }
                    pipeline._instantiated["clustering"] = {
                        "method": "centroid", 
                        "min_cluster_size": 12, 
                        "threshold": 0.7045654963945799
                    }
                
                print("   âœ… Pipeline ä¿®å¾©æˆåŠŸ")
            except Exception as fix_error:
                print(f"   âŒ Pipeline ä¿®å¾©å¤±æ•—: {fix_error}")
                raise
        
        # æ›´ç©æ¥µçš„è¨˜æ†¶é«”æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # ç¢ºä¿æ‰€æœ‰ CUDA æ“ä½œå®Œæˆ
            
        # å¼·åˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # åŸ·è¡Œ diarization
        print("   ğŸš€ é–‹å§‹åŸ·è¡Œ diarization...")
        with ProgressHook() as hook:
            diarization = pipeline(audio_file, hook=hook)
        
        # çµ±è¨ˆçµæœ
        speakers = set()
        total_duration = 0.0
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speakers.add(speaker)
            total_duration += turn.duration
        
        print(f"   âœ… Diarization completed: {len(speakers)} speakers, {total_duration:.1f}s total speech")
        
        return diarization
        
    except Exception as e:
        print(f"   âŒ Diarization failed: {e}")
        raise


def segment_audio_files(
    segments: List[Tuple[float, float, int]], 
    audio_path: str, 
    output_dir: str, 
    subtitles: List[Tuple[float, str]], 
    episode_num: int
) -> None:
    """Segment audio file and save with global speaker IDs"""
    
    print("4. Segmenting and saving audio files...")
    
    # è¼‰å…¥éŸ³æª”
    try:
        audio, sr = librosa.load(audio_path, sr=None)
        print(f"   âœ… Audio loaded: {len(audio)/sr:.1f}s, {sr}Hz")
    except Exception as e:
        print(f"   âŒ Error loading audio: {e}")
        return
    
    os.makedirs(output_dir, exist_ok=True)
    
    # å»ºç«‹å­—å¹•æŸ¥æ‰¾å­—å…¸
    subtitle_dict = {timestamp: text for timestamp, text in subtitles}
    
    saved_count = 0
    
    for i, (start, end, speaker_id) in enumerate(tqdm(segments, desc="   Saving segments", unit="seg", ncols=80)):
        try:
            # æå–éŸ³æª”ç‰‡æ®µ
            start_sample = int(start * sr)
            end_sample = int(end * sr)
            segment_audio = audio[start_sample:end_sample]
            
            if len(segment_audio) == 0:
                continue
            
            # å°‹æ‰¾å°æ‡‰çš„å­—å¹•
            segment_text = ""
            for timestamp, text in subtitles:
                if start <= timestamp < end:
                    segment_text += text + " "
            segment_text = segment_text.strip()
            
            if not segment_text:
                continue  # è·³éæ²’æœ‰å­—å¹•çš„ç‰‡æ®µ
            
            # å»ºç«‹æª”æ¡ˆè·¯å¾‘
            chapter_id = episode_num
            paragraph_id = i + 1
            sentence_id = 1
            
            utterance_id = f"{speaker_id:03d}_{chapter_id:03d}_{paragraph_id:06d}_{sentence_id:06d}"
            
            speaker_dir = os.path.join(output_dir, f"{speaker_id:03d}")
            chapter_dir = os.path.join(speaker_dir, f"{chapter_id:03d}")
            os.makedirs(chapter_dir, exist_ok=True)
            
            # å„²å­˜éŸ³æª”
            audio_filename = f"{utterance_id}.wav"
            audio_filepath = os.path.join(chapter_dir, audio_filename)
            sf.write(audio_filepath, segment_audio, sr)
            
            # å„²å­˜å­—å¹•
            text_filename = f"{utterance_id}.normalized.txt"
            text_filepath = os.path.join(chapter_dir, text_filename)
            with open(text_filepath, 'w', encoding='utf-8') as f:
                f.write(segment_text)
            
            saved_count += 1
            
        except Exception as e:
            print(f"   âš ï¸ Error saving segment {i}: {e}")
            continue
    
    print(f"   âœ… Saved {saved_count} audio segments to {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Pyannote Speaker Segmentation with Global Speaker Database")
    parser.add_argument("audio_file", help="Input audio file")
    parser.add_argument("subtitle_file", help="Input subtitle file (timestamp text format)")
    parser.add_argument("--episode_num", type=int, required=True, help="Episode number")
    parser.add_argument("--output_dir", default="output", help="Output directory")
    parser.add_argument("--min_duration", type=float, default=1.0, help="Minimum segment duration (seconds)")
    parser.add_argument("--max_duration", type=float, default=15.0, help="Maximum segment duration (seconds)")
    parser.add_argument("--similarity_threshold", type=float, default=0.40, help="Cosine similarity threshold for matching speakers")
    parser.add_argument("--voice_activity_threshold", type=float, default=0.1, help="Voice activity threshold for hybrid segmentation (0.0-1.0)")
    parser.add_argument("--min_speaker_duration", type=float, default=5.0, help="Minimum total duration for a speaker to be considered (seconds)")
    parser.add_argument("--force", action="store_true", help="Force reprocessing even if episode already processed")
    parser.add_argument("--device", choices=["cpu", "cuda"], default="cuda" if torch.cuda.is_available() else "cpu", help="Device to use")
    
    args = parser.parse_args()
    
    # è¨­å®šè£ç½®
    device = torch.device(args.device)
    print(f"Using device: {device}")
    
    # æª¢æŸ¥æª”æ¡ˆ
    if not os.path.exists(args.audio_file):
        print(f"âŒ Audio file not found: {args.audio_file}")
        sys.exit(1)
    
    if not os.path.exists(args.subtitle_file):
        print(f"âŒ Subtitle file not found: {args.subtitle_file}")
        sys.exit(1)
    
    # åˆå§‹åŒ–è³‡æ–™åº«
    print("1. Initializing speaker database...")
    db = SpeakerDatabase()
    
    # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
    processed_episodes = db.get_processed_episodes()
    if args.episode_num in processed_episodes and not args.force:
        print(f"   âš ï¸ Episode {args.episode_num} already processed. Use --force to reprocess.")
        sys.exit(0)
    
    # è¼‰å…¥å­—å¹•
    print("2. Loading subtitles...")
    subtitles = load_subtitles(args.subtitle_file)
    if not subtitles:
        print("âŒ No valid subtitles loaded")
        sys.exit(1)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("3. Initializing models...")
    
    # ğŸ¯ ç›´æ¥ä½¿ç”¨ HuggingFace æ¨™æº–æ¨¡å¼ï¼ˆä¾è³´å¿«å–ï¼‰
    try:
        print("   ğŸ” è¼‰å…¥ diarization pipelineï¼ˆHF æ¨™æº–é›¢ç·šæ¨¡å¼ï¼‰...")
        
        # ç›´æ¥ä½¿ç”¨ repo IDï¼Œä¾è³´ HF_HUB_CACHE è¨­å®š
        diarization_pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=None
        )
        
        diarization_pipeline = diarization_pipeline.to(device)
        print("   âœ… Diarization pipeline loadedï¼ˆå¾ HF å¿«å–ï¼‰")
        
        # è¨˜æ†¶é«”æ¸…ç†å’Œå„ªåŒ–
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"   ğŸ§¹ GPU è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")
            
    except Exception as e:
        print(f"   âŒ Error loading diarization pipeline: {e}")
        print(f"   ğŸ’¡ è«‹ç¢ºèª HuggingFace å¿«å–è·¯å¾‘: {os.environ.get('HF_HUB_CACHE')}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # ğŸ”§ è¨˜æ†¶é«”å„ªåŒ–ï¼šç›£æ§è¨˜æ†¶é«”ä½¿ç”¨ï¼ˆç°¡åŒ–ç‰ˆï¼‰
    print("4. Performing speaker diarization...")
    
    # æª¢æŸ¥ pipeline ç‹€æ…‹
    print(f"   ğŸ“Š Pipeline é¡å‹: {type(diarization_pipeline)}")
    print("   âœ… Pipeline æº–å‚™å°±ç·’")
    
    diarization = perform_speaker_diarization(args.audio_file, diarization_pipeline, device)
    
    # é‡‹æ”¾ diarization pipeline è¨˜æ†¶é«”
    print("   ğŸ§¹ é‡‹æ”¾ diarization pipeline è¨˜æ†¶é«”...")
    del diarization_pipeline
    
    # æ›´ç©æ¥µçš„è¨˜æ†¶é«”æ¸…ç†
    for _ in range(3):  # å¤šæ¬¡æ¸…ç†ç¢ºä¿è¨˜æ†¶é«”é‡‹æ”¾
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    print("   ğŸ’¾ Pipeline è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")
    
    # ç¾åœ¨æ‰è¼‰å…¥ embedding modelï¼ˆè¨˜æ†¶é«”ä½¿ç”¨åˆ†æ•£ï¼‰
    print("5. Loading embedding model...")
    try:
        embedding_inference = EmbeddingInference(device)
        print("   âœ… Embedding model loaded")
        
        # å†æ¬¡æ¸…ç†è¨˜æ†¶é«”
        for _ in range(2):
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
    except Exception as e:
        print(f"   âŒ Error loading embedding model: {e}")
        sys.exit(1)
    
    # åŸ·è¡Œèªªè©±äººç´šåˆ¥åˆ†æ®µ
    print("6. ğŸ¯ ä½¿ç”¨èªªè©±äººç´šåˆ¥åˆ†æ®µæ¨¡å¼")
    segments, local_to_global_map = segment_by_speaker_level_approach(
        diarization, subtitles, args.audio_file, embedding_inference.model, device,
        db, args.episode_num, args.min_duration, args.max_duration, args.similarity_threshold,
        args.min_speaker_duration
    )
    print(f"   âœ… å‰µå»ºäº† {len(segments)} å€‹èªªè©±äººç´šåˆ¥åˆ†æ®µ")
    
    if not segments:
        print("âŒ No valid segments created")
        sys.exit(1)
    
    # åˆ†å‰²ä¸¦å„²å­˜éŸ³æª”
    segment_audio_files(segments, args.audio_file, args.output_dir, subtitles, args.episode_num)
    
    # æ¨™è¨˜é›†æ•¸ç‚ºå·²è™•ç†
    db.mark_episode_processed(args.episode_num)
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    print("\n" + "="*60)
    print("ğŸ“Š Processing Summary")
    print("="*60)
    print(f"Episode: {args.episode_num}")
    print(f"Total Segments: {len(segments)}")
    print(f"Unique Speakers: {len(set(seg[2] for seg in segments))}")
    print(f"Similarity Threshold: {args.similarity_threshold}")
    print(f"Min Speaker Duration: {args.min_speaker_duration}s")
    
    if local_to_global_map:
        print(f"\nSpeaker Mapping:")
        for local_label, global_id in sorted(local_to_global_map.items()):
            print(f"  {local_label} â†’ Global Speaker {global_id}")
    
    # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
    db_stats = db.get_database_stats()
    print(f"\nDatabase Stats:")
    print(f"  Total Speakers: {db_stats['total_speakers']}")
    print(f"  Total Episodes: {db_stats['total_episodes']}")
    print(f"  Total Segments: {db_stats['total_segments']}")
    
    print(f"\nâœ… Processing completed successfully!")
    print(f"Output saved to: {args.output_dir}")


if __name__ == "__main__":
    main()
