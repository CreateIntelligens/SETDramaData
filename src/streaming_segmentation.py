#!/usr/bin/env python3
"""
Streaming-style speaker segmentation
é€£çºŒæ±ºç­–çš„speakeråˆ†æ®µï¼Œä¸åšè·³èºåˆä½µ
"""

import numpy as np
from typing import List, Tuple, Optional, Dict
from pyannote.core import Annotation, Segment
import torch


def segment_by_streaming_decision(
    diarization: Annotation,
    audio_path: str,
    embedding_model,
    device: torch.device,
    db,  # è³‡æ–™åº«ç‰©ä»¶
    episode_num: int,
    min_duration: float = 2.0,
    max_duration: float = 15.0,
    similarity_threshold: float = 0.85
) -> Tuple[List[Tuple[float, float, int]], Dict[str, int]]:
    """
    æŒ‰æ™‚é–“é †åºé€£çºŒæ±ºç­–åˆ‡åˆ†ï¼Œå³æ™‚è¨»å†Šæ–°speaker
    è¿”å›: (segments_list, local_to_global_mapping)
    """
    
    # æŒ‰æ™‚é–“æ’åºæ‰€æœ‰è®ŠåŒ–é»
    timeline = []
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        timeline.append((turn.start, turn.end, speaker))
    
    timeline.sort(key=lambda x: x[0])
    print(f"   ğŸ“… Timeline: {len(timeline)} speaker changes")
    
    segments = []
    current_segment = None
    local_to_global_map = {}
    
    def get_or_register_speaker(local_label: str, start_time: float, end_time: float) -> int:
        """å³æ™‚ç²å–æˆ–è¨»å†Šspeakerçš„global ID"""
        if local_label in local_to_global_map:
            return local_to_global_map[local_label]
        
        # æå–é€™æ®µçš„embedding
        print(f"     ğŸ§¬ Extracting embedding for new speaker {local_label}")
        segment_dict = {
            "audio": audio_path,
            "start": start_time,
            "end": end_time
        }
        
        try:
            with torch.no_grad():
                embedding = embedding_model(segment_dict)
            
            # Handle different types of embedding outputs
            if hasattr(embedding, 'cpu'):
                embedding_np = embedding.cpu().numpy()
            elif isinstance(embedding, dict):
                # Handle case where embedding is returned as dict
                if 'embeddings' in embedding:
                    embedding_np = embedding['embeddings']
                elif 'embedding' in embedding:
                    embedding_np = embedding['embedding']
                else:
                    # Take the first value if it's a dict
                    embedding_np = list(embedding.values())[0]
                
                # Convert to numpy if still tensor
                if hasattr(embedding_np, 'cpu'):
                    embedding_np = embedding_np.cpu().numpy()
            else:
                embedding_np = embedding
            
            print(f"     ğŸ“Š Embedding shape: {embedding_np.shape}, Mean: {np.mean(embedding_np):.4f}")
            
            # Debug: Check embedding dimensions
            if embedding_np.ndim == 1:
                print(f"     âœ“ Valid 1D embedding: {embedding_np.shape[0]} features")
            else:
                print(f"     âš ï¸ Unexpected embedding shape: {embedding_np.shape}, reshaping to 1D")
                embedding_np = embedding_np.flatten()
            
            # ç«‹å³åœ¨è³‡æ–™åº«ä¸­æŸ¥æ‰¾æˆ–è¨»å†Š
            speaker_id, similarity = db.find_similar_speaker(embedding_np, similarity_threshold)
            
            if speaker_id is not None:
                print(f"     ğŸ” Matched existing Global Speaker ID: {speaker_id} (Similarity: {similarity:.3f})")
                db.update_speaker_episode(speaker_id, episode_num, local_label, 1)
            else:
                speaker_id = db.add_speaker(embedding_np, episode_num, local_label, 1)
                print(f"     âœ¨ Registered new Global Speaker ID: {speaker_id}")
            
            local_to_global_map[local_label] = speaker_id
            return speaker_id
            
        except Exception as e:
            print(f"     âš ï¸ Embedding extraction failed: {e}")
            # fallback: ç›´æ¥è¨»å†Šæ²’æœ‰embeddingçš„speaker
            speaker_id = db.add_speaker(np.zeros(512), episode_num, local_label, 1)
            local_to_global_map[local_label] = speaker_id
            return speaker_id
    
    for i, (start, end, speaker_label) in enumerate(timeline):
        print(f"   ğŸ¯ Processing {speaker_label} at {start:.1f}-{end:.1f}s")
        
        # ç«‹å³ç²å–é€™å€‹speakerçš„global ID
        global_speaker_id = get_or_register_speaker(speaker_label, start, end)
        
        if current_segment is None:
            # ç¬¬ä¸€æ®µï¼Œç›´æ¥é–‹å§‹
            current_segment = {
                'start': start,
                'end': end, 
                'speaker_label': speaker_label,
                'global_id': global_speaker_id,
                'segments_count': 1
            }
            print(f"     âœ¨ Starting new segment with Global ID {global_speaker_id}")
            continue
        
        # æª¢æŸ¥æ˜¯å¦èˆ‡ç•¶å‰æ®µé€£çºŒ
        is_continuous = abs(current_segment['end'] - start) < 0.1  # å®¹å¿0.1ç§’èª¤å·®
        is_same_global_speaker = current_segment['global_id'] == global_speaker_id
        
        # æª¢æŸ¥åˆä½µå¾Œæ˜¯å¦è¶…éæœ€å¤§é•·åº¦
        merged_duration = end - current_segment['start']
        within_max_duration = merged_duration <= max_duration
        
        if is_continuous and is_same_global_speaker and within_max_duration:
            # æ¢ä»¶ï¼šé€£çºŒ + åŒä¸€global speaker + ä¸è¶…é•· â†’ åˆä½µ
            print(f"     âœ… Merging with Global ID {global_speaker_id} ({merged_duration:.1f}s)")
            current_segment['end'] = end
            current_segment['segments_count'] += 1
            print(f"     ğŸ“ˆ Extended to {current_segment['end']:.1f}s ({current_segment['segments_count']} parts)")
            continue
        
        # ä¸èƒ½åˆä½µï¼ŒçµæŸç•¶å‰segmentï¼Œé–‹å§‹æ–°çš„
        if current_segment['end'] - current_segment['start'] >= min_duration:
            segments.append((
                current_segment['start'], 
                current_segment['end'], 
                current_segment['global_id']  # ä½¿ç”¨global ID
            ))
            print(f"     ğŸ’¾ Saved segment: Global ID {current_segment['global_id']} ({current_segment['end'] - current_segment['start']:.1f}s)")
        else:
            print(f"     ğŸ—‘ï¸ Discarded short segment: {current_segment['end'] - current_segment['start']:.1f}s")
        
        # é–‹å§‹æ–°segment
        current_segment = {
            'start': start,
            'end': end,
            'speaker_label': speaker_label,
            'global_id': global_speaker_id,
            'segments_count': 1
        }
        print(f"     ğŸ†• New segment: Global ID {global_speaker_id}")
    
    # è™•ç†æœ€å¾Œä¸€æ®µ
    if current_segment and (current_segment['end'] - current_segment['start']) >= min_duration:
        segments.append((
            current_segment['start'], 
            current_segment['end'], 
            current_segment['global_id']
        ))
        print(f"   ğŸ’¾ Final segment: Global ID {current_segment['global_id']} ({current_segment['end'] - current_segment['start']:.1f}s)")
    
    print(f"   âœ… Created {len(segments)} final segments")
    return segments, local_to_global_map


def verify_continuous_segments(
    audio_path: str,
    seg1_start: float, seg1_end: float,
    seg2_start: float, seg2_end: float,
    embedding_model,
    device: torch.device,
    threshold: float = 0.85
) -> bool:
    """
    é©—è­‰å…©å€‹é€£çºŒsegmentsæ˜¯å¦ç‚ºåŒä¸€speaker
    """
    try:
        embeddings = []
        
        # å–æ¯æ®µçš„ä¸­é–“éƒ¨åˆ†ä¾†æ¯”è¼ƒ
        for start, end in [(seg1_start, seg1_end), (seg2_start, seg2_end)]:
            if end - start < 0.5:  # å¤ªçŸ­çš„æ®µè½
                return False
            
            # å–ä¸­é–“80%çš„éƒ¨åˆ†
            duration = end - start
            margin = duration * 0.1
            safe_start = start + margin
            safe_end = end - margin
            
            segment_dict = {
                "audio": audio_path,
                "start": safe_start,
                "end": safe_end
            }
            
            try:
                with torch.no_grad():
                    embedding = embedding_model(segment_dict)
                
                if hasattr(embedding, 'cpu'):
                    embeddings.append(embedding.cpu().numpy())
                else:
                    embeddings.append(embedding)
                    
            except Exception as e:
                print(f"     âš ï¸ Embedding extraction failed: {e}")
                return True  # å¦‚æœæå–å¤±æ•—ï¼Œä¿å®ˆåœ°å…è¨±åˆä½µ
        
        if len(embeddings) == 2:
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarity_tensor = torch.nn.functional.cosine_similarity(
                torch.tensor(embeddings[0]).unsqueeze(0),
                torch.tensor(embeddings[1]).unsqueeze(0)
            )
            similarity = similarity_tensor.item() if similarity_tensor.numel() == 1 else similarity_tensor[0].item()
            
            print(f"     ğŸ“Š Similarity: {similarity:.3f}")
            return similarity > threshold
            
        return False
        
    except Exception as e:
        print(f"     âš ï¸ Verification failed: {e}")
        return True  # å‡ºéŒ¯æ™‚ä¿æŒåˆä½µ