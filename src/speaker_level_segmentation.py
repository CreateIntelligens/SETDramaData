#!/usr/bin/env python3
"""
èªªè©±äººç´šåˆ¥åˆ†æ®µç³»çµ± - å…©éšæ®µèªªè©±äººè­˜åˆ¥
éšæ®µ1ï¼šå–®é›†å…§èªªè©±äººæ•´åˆ - åˆä½µåŒèªªè©±äººç‰‡æ®µï¼Œæå–èªªè©±äººç´šåˆ¥çš„ embedding
éšæ®µ2ï¼šè·¨é›†èªªè©±äººåŒ¹é… - èˆ‡å…¨åŸŸè³‡æ–™åº«æ¯”å°ï¼Œåˆ†é… Global Speaker ID
"""

import os
import numpy as np
from typing import List, Tuple, Dict, Optional
import torch
from tqdm import tqdm
import librosa
import soundfile as sf
from pyannote.core import Annotation, Segment
from collections import defaultdict


def segment_by_speaker_level_approach(
    diarization: Annotation,
    subtitles: List[Tuple[float, str]],
    audio_path: str,
    embedding_model,
    device: torch.device,
    db,
    episode_num: int,
    min_duration: float = 1.0,
    max_duration: float = 15.0,
    similarity_threshold: float = 0.40,
    min_speaker_duration: float = 5.0
) -> Tuple[List[Tuple[float, float, int]], Dict[str, int]]:
    """
    èªªè©±äººç´šåˆ¥åˆ†æ®µæ–¹æ³•ï¼šå…©éšæ®µèªªè©±äººè­˜åˆ¥
    
    éšæ®µ1ï¼šå–®é›†å…§èªªè©±äººæ•´åˆ
    1. ä½¿ç”¨ diarization çµæœè­˜åˆ¥èªªè©±äººè®ŠåŒ–é»
    2. åˆä½µåŒèªªè©±äººçš„æ‰€æœ‰ç‰‡æ®µ
    3. å°æ¯å€‹èªªè©±äººçš„å®Œæ•´éŸ³æª”æå– embedding
    
    éšæ®µ2ï¼šè·¨é›†èªªè©±äººåŒ¹é…
    1. å°‡èªªè©±äºº embedding èˆ‡è³‡æ–™åº«æ¯”å°
    2. åˆ†é… Global Speaker ID
    3. æ ¹æ“šå­—å¹•æ™‚é–“é»ç”Ÿæˆæœ€çµ‚åˆ†æ®µ
    """
    
    if not subtitles:
        print("   âŒ æ²’æœ‰å­—å¹•è³‡æ–™")
        return [], {}
    
    if not diarization:
        print("   âŒ æ²’æœ‰ diarization çµæœ")
        return [], {}
    
    print(f"   ğŸ¯ èªªè©±äººç´šåˆ¥åˆ†æ®µæ¨¡å¼ï¼š{len(subtitles)} å¥å­—å¹• + diarization èªªè©±äººè³‡è¨Š")
    
    # éšæ®µ1ï¼šå–®é›†å…§èªªè©±äººæ•´åˆ
    print("   ğŸ“Š éšæ®µ1ï¼šå–®é›†å…§èªªè©±äººæ•´åˆ")
    speaker_segments = extract_speaker_segments_from_diarization(diarization, audio_path)
    print(f"   âœ‚ï¸ å¾ diarization æå–äº† {len(speaker_segments)} å€‹èªªè©±äºº")
    
    # éæ¿¾å¤ªçŸ­çš„èªªè©±äºº
    valid_speakers = {speaker: segments for speaker, segments in speaker_segments.items() 
                     if calculate_total_duration(segments) >= min_speaker_duration}
    filtered_count = len(speaker_segments) - len(valid_speakers)
    if filtered_count > 0:
        print(f"   ğŸ—‘ï¸ éæ¿¾äº† {filtered_count} å€‹èªªè©±æ™‚é–“å¤ªçŸ­çš„èªªè©±äºº (< {min_speaker_duration}s)")
    
    # ç‚ºæ¯å€‹èªªè©±äººæå– embedding
    speaker_embeddings = extract_speaker_level_embeddings(
        valid_speakers, audio_path, embedding_model, device
    )
    print(f"   ğŸ§¬ æˆåŠŸæå–äº† {len(speaker_embeddings)} å€‹èªªè©±äººçš„ embedding")
    
    # éšæ®µ2ï¼šè·¨é›†èªªè©±äººåŒ¹é…
    print("   ğŸ” éšæ®µ2ï¼šè·¨é›†èªªè©±äººåŒ¹é…")
    local_to_global_map = assign_global_speaker_ids_by_embedding(
        speaker_embeddings, valid_speakers, db, episode_num, similarity_threshold
    )
    
    # ç”Ÿæˆæœ€çµ‚åˆ†æ®µï¼ˆåŸºæ–¼å­—å¹•æ™‚é–“é»ï¼‰
    print("   ğŸ“ ç”Ÿæˆæœ€çµ‚åˆ†æ®µ")
    final_segments = generate_final_segments_with_subtitles(
        subtitles, diarization, local_to_global_map, min_duration, max_duration
    )
    
    print(f"   âœ… æœ€çµ‚ç”¢ç”Ÿ {len(final_segments)} å€‹æœ‰æ•ˆæ®µè½")
    return final_segments, local_to_global_map


def extract_speaker_segments_from_diarization(
    diarization: Annotation, 
    audio_path: str
) -> Dict[str, List[Segment]]:
    """
    å¾ diarization çµæœä¸­æå–æ¯å€‹èªªè©±äººçš„æ‰€æœ‰ç‰‡æ®µ
    è¿”å›ï¼š{speaker_label: [Segment1, Segment2, ...]}
    """
    speaker_segments = defaultdict(list)
    
    print("   ğŸ­ å¾ diarization æå–èªªè©±äººç‰‡æ®µ...")
    
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speaker_segments[speaker].append(turn)
    
    # çµ±è¨ˆæ¯å€‹èªªè©±äººçš„è³‡è¨Š
    for speaker, segments in speaker_segments.items():
        total_duration = sum(seg.duration for seg in segments)
        print(f"     {speaker}: {len(segments)} å€‹ç‰‡æ®µ, ç¸½æ™‚é•· {total_duration:.1f}s")
    
    return dict(speaker_segments)


def calculate_total_duration(segments: List[Segment]) -> float:
    """è¨ˆç®—ç‰‡æ®µåˆ—è¡¨çš„ç¸½æ™‚é•·"""
    return sum(seg.duration for seg in segments)


def extract_speaker_level_embeddings(
    speaker_segments: Dict[str, List[Segment]],
    audio_path: str,
    embedding_model,
    device: torch.device
) -> Dict[str, np.ndarray]:
    """
    ç‚ºæ¯å€‹èªªè©±äººæå–ä»£è¡¨æ€§çš„ embedding
    ä½¿ç”¨å®Œæ•´çš„èªªè©±æ®µè½è€ŒéçŸ­ç‰‡æ®µ
    """
    print("   ğŸ”Š è¼‰å…¥éŸ³æª”é€²è¡Œ embedding æå–...")
    try:
        waveform, sr = librosa.load(audio_path, sr=16000)
    except Exception as e:
        print(f"   âŒ ç„¡æ³•è¼‰å…¥éŸ³æª”: {e}")
        return {}
    
    speaker_embeddings = {}
    
    print("   ğŸ§¬ ç‚ºæ¯å€‹èªªè©±äººæå– embedding...")
    for speaker, segments in tqdm(speaker_segments.items(), desc="   æå–èªªè©±äºº embedding", unit="speaker", ncols=80):
        # åˆä½µæ‰€æœ‰å±¬æ–¼è©²èªªè©±äººçš„éŸ³æª”ç‰‡æ®µ
        combined_audio = combine_speaker_audio_segments(waveform, sr, segments)
        
        if combined_audio.size < sr * 1.0:  # è‡³å°‘éœ€è¦1ç§’çš„éŸ³æª”
            print(f"     âš ï¸ {speaker}: éŸ³æª”å¤ªçŸ­ ({combined_audio.size/sr:.1f}s)ï¼Œè·³é")
            continue
        
        # æå– embedding
        try:
            embedding = extract_embedding_from_audio(combined_audio, embedding_model, device)
            if embedding is not None:
                speaker_embeddings[speaker] = embedding
                print(f"     âœ… {speaker}: embedding æå–æˆåŠŸ ({combined_audio.size/sr:.1f}s éŸ³æª”)")
            else:
                print(f"     âŒ {speaker}: embedding æå–å¤±æ•—")
        except Exception as e:
            print(f"     âŒ {speaker}: embedding æå–éŒ¯èª¤ - {e}")
    
    return speaker_embeddings


def combine_speaker_audio_segments(
    waveform: np.ndarray, 
    sr: int, 
    segments: List[Segment]
) -> np.ndarray:
    """
    åˆä½µåŒä¸€èªªè©±äººçš„æ‰€æœ‰éŸ³æª”ç‰‡æ®µ
    è¿”å›åˆä½µå¾Œçš„éŸ³æª”æ•¸æ“š
    """
    combined_segments = []
    
    for segment in segments:
        start_sample = int(segment.start * sr)
        end_sample = int(segment.end * sr)
        
        # ç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
        start_sample = max(0, start_sample)
        end_sample = min(len(waveform), end_sample)
        
        if end_sample > start_sample:
            segment_audio = waveform[start_sample:end_sample]
            combined_segments.append(segment_audio)
    
    if combined_segments:
        # å°‡æ‰€æœ‰ç‰‡æ®µä¸²æ¥èµ·ä¾†
        return np.concatenate(combined_segments)
    else:
        return np.array([])


def extract_embedding_from_audio(
    audio_data: np.ndarray,
    embedding_model,
    device: torch.device
) -> Optional[np.ndarray]:
    """
    å¾éŸ³æª”æ•¸æ“šæå– embedding
    """
    try:
        # è½‰æ›ç‚º tensor
        audio_tensor = torch.from_numpy(audio_data).unsqueeze(0).to(device)
        
        with torch.no_grad():
            embedding_output = embedding_model(audio_tensor)
        
        # è™•ç†è¼¸å‡º
        if hasattr(embedding_output, 'cpu'):
            embedding = embedding_output.cpu().numpy()[0].flatten()
        else:
            embedding = np.array(embedding_output)[0].flatten()
        
        return embedding.astype(np.float32)
        
    except Exception as e:
        print(f"       âŒ Embedding æå–éŒ¯èª¤: {e}")
        return None


def assign_global_speaker_ids_by_embedding(
    speaker_embeddings: Dict[str, np.ndarray],
    speaker_segments: Dict[str, List[Segment]],
    db,
    episode_num: int,
    similarity_threshold: float
) -> Dict[str, int]:
    """
    åŸºæ–¼ embedding ç›¸ä¼¼åº¦åˆ†é… Global Speaker ID
    """
    local_to_global_map = {}
    
    print(f"   ğŸ¯ ç‚º {len(speaker_embeddings)} å€‹èªªè©±äººåˆ†é… Global Speaker ID")
    print(f"   ğŸ” ç›¸ä¼¼åº¦é–¾å€¼: {similarity_threshold}")
    
    for local_speaker, embedding in speaker_embeddings.items():
        segments = speaker_segments[local_speaker]
        total_duration = calculate_total_duration(segments)
        segment_count = len(segments)
        
        print(f"     è™•ç†èªªè©±äºº {local_speaker}: {segment_count} å€‹ç‰‡æ®µ, ç¸½æ™‚é•· {total_duration:.1f}s")
        
        try:
            # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ embedding æ›´æ–°
            update_embeddings = os.getenv("UPDATE_SPEAKER_EMBEDDINGS", "false").lower() == "true"
            update_weight = float(os.getenv("EMBEDDING_UPDATE_WEIGHT", "1.0"))
            
            # åœ¨è³‡æ–™åº«ä¸­æŸ¥æ‰¾ç›¸ä¼¼èªªè©±äºº
            speaker_id, similarity = db.find_similar_speaker(
                embedding, similarity_threshold, update_embeddings, update_weight
            )
            
            if speaker_id is not None:
                print(f"       ğŸ” åŒ¹é…åˆ°ç¾æœ‰èªªè©±äºº: Global ID {speaker_id} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                # æ›´æ–°èªªè©±äººåœ¨æ­¤é›†æ•¸çš„å‡ºç¾è¨˜éŒ„
                db.update_speaker_episode(speaker_id, episode_num, local_speaker, segment_count)
            else:
                # è¨»å†Šæ–°èªªè©±äºº
                speaker_id = db.add_speaker(embedding, episode_num, local_speaker, segment_count)
                print(f"       âœ¨ è¨»å†Šæ–°èªªè©±äºº: Global ID {speaker_id}")
            
            local_to_global_map[local_speaker] = speaker_id
            
        except Exception as e:
            print(f"       âŒ èªªè©±äºº {local_speaker} ID åˆ†é…å¤±æ•—: {e}")
            continue
    
    print(f"   ğŸ“Š æˆåŠŸåˆ†é… {len(local_to_global_map)} å€‹èªªè©±äººçš„ Global ID")
    return local_to_global_map


def generate_final_segments_with_subtitles(
    subtitles: List[Tuple[float, str]],
    diarization: Annotation,
    local_to_global_map: Dict[str, int],
    min_duration: float,
    max_duration: float
) -> List[Tuple[float, float, int]]:
    """
    åŸºæ–¼å­—å¹•æ™‚é–“é»ç”Ÿæˆæœ€çµ‚åˆ†æ®µï¼Œä¸¦åˆ†é… Global Speaker ID
    """
    final_segments = []
    
    print("   ğŸ“ åŸºæ–¼å­—å¹•ç”Ÿæˆæœ€çµ‚åˆ†æ®µ...")
    
    for i, (start_time, text) in enumerate(tqdm(subtitles, desc="   è™•ç†å­—å¹•", unit="line", ncols=80)):
        # è¨ˆç®—çµæŸæ™‚é–“
        if i < len(subtitles) - 1:
            end_time = subtitles[i + 1][0]
        else:
            # æœ€å¾Œä¸€å¥ï¼Œä¼°ç®—çµæŸæ™‚é–“
            end_time = start_time + min(10.0, max_duration)
        
        # é™åˆ¶æœ€å¤§é•·åº¦
        end_time = min(end_time, start_time + max_duration)
        duration = end_time - start_time
        
        # éæ¿¾é•·åº¦
        if duration < min_duration or duration > max_duration:
            continue
        
        # æ‰¾åˆ°é€™å€‹æ™‚é–“ç¯„åœå…§çš„ä¸»è¦èªªè©±äºº
        segment_range = Segment(start_time, end_time)
        dominant_speaker = get_dominant_speaker_in_range(diarization, segment_range)
        
        if dominant_speaker and dominant_speaker in local_to_global_map:
            global_speaker_id = local_to_global_map[dominant_speaker]
            final_segments.append((start_time, end_time, global_speaker_id))
    
    return final_segments


def get_dominant_speaker_in_range(diarization: Annotation, segment: Segment) -> Optional[str]:
    """
    åœ¨çµ¦å®šæ™‚é–“ç¯„åœå…§æ‰¾åˆ°èªªè©±æ™‚é–“æœ€é•·çš„èªªè©±äºº
    """
    speaker_durations = {}
    
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        overlap = segment & turn
        if overlap:
            duration = overlap.duration
            if speaker in speaker_durations:
                speaker_durations[speaker] += duration
            else:
                speaker_durations[speaker] = duration
    
    if not speaker_durations:
        return None
    
    return max(speaker_durations.items(), key=lambda x: x[1])[0]
